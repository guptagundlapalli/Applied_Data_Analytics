{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptagundlapalli/Applied_Data_Analytics/blob/master/SqueezeLLM_Make_Accurate_Quantized_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows how to quantize Llama 2 with SqueezeLLM to 3-bit and 4-bit.\n",
        "\n",
        "You can follow the same steps for Mistral 7B, but set \"mistral\" for \"--model_type\".\n",
        "\n",
        "\n",
        "First, install SqueezeLLM (you need a GPU):"
      ],
      "metadata": {
        "id": "12RmrQVu0Cn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOsl-HJiLhLW",
        "outputId": "24e322c9-b8cd-4e62-d9ef-6246a9167220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SqueezeLLM' already exists and is not an empty directory.\n",
            "Obtaining file:///content/SqueezeLLM\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from squeezellm==0.1.0) (0.27.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from squeezellm==0.1.0) (0.1.99)\n",
            "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from squeezellm==0.1.0) (0.13.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from squeezellm==0.1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers==4.29.0 in /usr/local/lib/python3.10/dist-packages (from squeezellm==0.1.0) (4.29.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from squeezellm==0.1.0) (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->squeezellm==0.1.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->squeezellm==0.1.0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->squeezellm==0.1.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->squeezellm==0.1.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->squeezellm==0.1.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->squeezellm==0.1.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->squeezellm==0.1.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->squeezellm==0.1.0) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->squeezellm==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->squeezellm==0.1.0) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->squeezellm==0.1.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->squeezellm==0.1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->squeezellm==0.1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->squeezellm==0.1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->squeezellm==0.1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->squeezellm==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->squeezellm==0.1.0) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->squeezellm==0.1.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->squeezellm==0.1.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->squeezellm==0.1.0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->squeezellm==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->squeezellm==0.1.0) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->squeezellm==0.1.0) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->squeezellm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->squeezellm==0.1.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->squeezellm==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->squeezellm==0.1.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->squeezellm==0.1.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->squeezellm==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.0->squeezellm==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.0->squeezellm==0.1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.0->squeezellm==0.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.0->squeezellm==0.1.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->squeezellm==0.1.0) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->squeezellm==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->squeezellm==0.1.0) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->squeezellm==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->squeezellm==0.1.0) (1.16.0)\n",
            "Building wheels for collected packages: squeezellm\n",
            "  Building editable for squeezellm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for squeezellm: filename=squeezellm-0.1.0-0.editable-py3-none-any.whl size=7572 sha256=bdaba6748c6e8c09add7dba921a476fa0250c7b649ec8740b72c72c68b444753\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xtnlmzzp/wheels/30/97/8c/2919c99f23ac8acc2921a339f74b1c532d5f2ad69abeca7ac8\n",
            "Successfully built squeezellm\n",
            "Installing collected packages: squeezellm\n",
            "  Attempting uninstall: squeezellm\n",
            "    Found existing installation: squeezellm 0.1.0\n",
            "    Uninstalling squeezellm-0.1.0:\n",
            "      Successfully uninstalled squeezellm-0.1.0\n",
            "Successfully installed squeezellm-0.1.0\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing quant_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to quant_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to quant_cuda.egg-info/top_level.txt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest file 'quant_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'quant_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for quant_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/quant_cuda.py to quant_cuda.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying quant_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.quant_cuda.cpython-310: module references __file__\n",
            "creating 'dist/quant_cuda-0.0.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "Extracting quant_cuda-0.0.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "quant-cuda 0.0.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for quant-cuda==0.0.0\n",
            "Finished processing dependencies for quant-cuda==0.0.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SqueezeAILab/SqueezeLLM\n",
        "!cd SqueezeLLM && pip install -e .\n",
        "!cd SqueezeLLM/squeezellm && python setup_cuda.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Llama 2 from the Hugging Face. Make sure you have your Hugging Face access token registered locally or run the following cell."
      ],
      "metadata": {
        "id": "9_Moyxr2BJNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "eee3d51eba8948d2a8b7e997378409a8",
            "f1c0d8ccd54e4f86a720115a3df2c805",
            "a2fff39d908342d6b39dec223ececb87",
            "a9281ca81d7b4148874867c73cea14ca",
            "59f22d232fec465ea3e9843d9eaafee5",
            "063abb89c097488fbb87b4c2175bff8b",
            "96a0b31e5217441d975ef5b066094bd6",
            "1e36e38a9e4141ff93125c54859d7eab",
            "dac2f0a09a744be89c8db02916a34bde",
            "bdfce33dd9ee44ea878fcb8df0c3fd01",
            "05d17a2bcf4045f1b143349336c15ef1",
            "816a602e65644e128df40bf1f549e418",
            "a9b812b80cc54c7aa050ee810756c339",
            "b112918b61c741819bb138de0bc20565",
            "bc0d8fa2739b4bcb9539b63ba6f5bcc7",
            "746a2281325f4541951d2312278889e5",
            "57d6b3715b3b4e1988cf8976f33ebc37",
            "81b8bf43b1a241e598c939bde73fb552",
            "06d186ead076463294f0e04be06c4cd9",
            "fbe4cb5033a14a578d98ed3ec1ee0479",
            "1b022bb73c7c4fd58082eda3445cfab1",
            "9a521189b23b4e5c8ed359d61fb13bc1",
            "d0bbe402729f47bebe07db37d3553aac",
            "e632dd933c3d482dae36b8ce5a75a7c1",
            "aa95fb9a1696497cb9592c0f787aa23e",
            "17afd077c02e458a821562bcf8e592cc",
            "bae0a86d49094b9cbd4a0027b0e5cbd7",
            "4656b587da364bbcb6bbc5bd1ab00252",
            "8b4ccc60c6374ae5b1bb190c5d5f6594",
            "d3b7e58a709c41cb96c6ee51e12006e9",
            "5c59ca20bcbb45fe830ccef5316c2812",
            "c9c42488223d4536af899f52c5ff7e4a"
          ]
        },
        "id": "OIv4YgtDBIsc",
        "outputId": "f1604aca-dea1-4308-af31-61d0cab0ca79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eee3d51eba8948d2a8b7e997378409a8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpNk0Lp5Mkdm"
      },
      "source": [
        "For computing the gradients used in the non-uniform quantization, we must also install the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvEX8JB1MmNU",
        "outputId": "da671d40-66d1-4183-a845-0185434ca38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SqueezeLLM-gradients'...\n",
            "remote: Enumerating objects: 155017, done.\u001b[K\n",
            "remote: Counting objects: 100% (499/499), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 155017 (delta 475), reused 477 (delta 465), pack-reused 154518\u001b[K\n",
            "Receiving objects: 100% (155017/155017), 157.30 MiB | 35.86 MiB/s, done.\n",
            "Resolving deltas: 100% (119044/119044), done.\n",
            "Obtaining file:///content/SqueezeLLM-gradients\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.36.0.dev0-0.editable-py3-none-any.whl size=9737 sha256=165465a2fbd773deb20e855c44b82bd07c549efc3af3cd02bf461ef90659e7aa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2omnikno/wheels/b5/7a/38/ca28f74cf0120ac333362e3b573b9d12564b84792ea146acad\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.36.0.dev0\n",
            "Collecting torch==2.0.1 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.14.5 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (10.0.1)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.5->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (3.4.1)\n",
            "Collecting multiprocess (from datasets==2.14.5->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1)) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1))\n",
            "  Downloading lit-17.0.6.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 1)) (2.1.5)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.14.5->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.5->-r requirements.txt (line 2)) (1.16.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.6-py3-none-any.whl size=93255 sha256=8eb271f7eaf311b0411036e243c8408f7b4f1a969e4419bf488983581bb4bbe2\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/dd/04/47d42976a6a86dc2ab66d7518621ae96f43452c8841d74758a\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, dill, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, datasets, triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.14.5 dill-0.3.7 lit-17.0.6 multiprocess-0.70.15 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kssteven418/SqueezeLLM-gradients.git\n",
        "!cd SqueezeLLM-gradients && pip install -e . && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we compute the gradients. This cell needs a GPU. If you use Google Colab, only the A100 will have enough memory."
      ],
      "metadata": {
        "id": "U22VjxYY0wNe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWyPP9LeM6mI",
        "outputId": "27c912cd-b3c2-4917-d3d6-f857bf8fe874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-08 04:25:08.290195: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-08 04:25:08.290243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-08 04:25:08.291882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-08 04:25:09.412821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Calibration with C4 \n",
            "Downloading readme: 100% 41.1k/41.1k [00:00<00:00, 4.30MB/s]\n",
            "Resolving data files: 100% 1024/1024 [00:13<00:00, 77.95it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:01<00:00, 955.43it/s]\n",
            "Resolving data files: 100% 7168/7168 [00:00<00:00, 387607.44it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 86285.91it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 377812.04it/s]\n",
            "Resolving data files: 100% 59024/59024 [00:00<00:00, 399212.10it/s]\n",
            "Resolving data files: 100% 386/386 [00:02<00:00, 158.35it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 157625.05it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 384302.73it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 261187.50it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 137026.78it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 353466.16it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 688.34it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 621.48it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 356576.78it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 353553.45it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 370128.17it/s]\n",
            "Resolving data files: 100% 2048/2048 [00:01<00:00, 1501.15it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 1506.11it/s]\n",
            "Resolving data files: 100% 11264/11264 [00:00<00:00, 365529.13it/s]\n",
            "Resolving data files: 100% 128/128 [00:01<00:00, 121.85it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 183107.41it/s]\n",
            "Resolving data files: 100% 2048/2048 [00:00<00:00, 2895.06it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 243865.96it/s]\n",
            "Resolving data files: 100% 64/64 [00:01<00:00, 63.74it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 229370.75it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 386064.48it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 266834.45it/s]\n",
            "Resolving data files: 100% 2048/2048 [00:01<00:00, 1530.91it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 310509.49it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 238609.29it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 378778.31it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:03<00:00, 275.09it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 99163.45it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 224749.73it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 127039.97it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 367342.40it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 1264.22it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:01<00:00, 822.17it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 340546.09it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 335020.85it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 260616.95it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 247405.95it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 354662.87it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 88768.34it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 74.01it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 202745.81it/s]\n",
            "Resolving data files: 100% 512/512 [00:01<00:00, 460.74it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 243534.09it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 142860.81it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 308901.56it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 127009.92it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 371826.45it/s]\n",
            "Resolving data files: 100% 512/512 [00:01<00:00, 341.48it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 114203.55it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 78283.89it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 353088.40it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 252052.07it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 223789.46it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 70271.06it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 238622.55it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 1153.08it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 230862.57it/s]\n",
            "Resolving data files: 100% 4096/4096 [00:01<00:00, 3139.03it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 209715.20it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 62339.86it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 254682.60it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 87896.35it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 220775.54it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 343048.51it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 255409.57it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 176428.17it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 259860.07it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 227765.14it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 207446.26it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 338399.57it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 308546.50it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 256140.70it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 375171.85it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 376817.63it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 377978.29it/s]\n",
            "Resolving data files: 100% 3072/3072 [00:00<00:00, 65344.89it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 209715.20it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 150468.30it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 65281.00it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 218473.34it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 373572.87it/s]\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/319M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   1% 4.19M/319M [00:00<00:27, 11.5MB/s]\u001b[A\n",
            "Downloading data:   4% 12.6M/319M [00:00<00:10, 29.6MB/s]\u001b[A\n",
            "Downloading data:   7% 21.0M/319M [00:00<00:07, 42.4MB/s]\u001b[A\n",
            "Downloading data:   9% 29.4M/319M [00:00<00:05, 52.3MB/s]\u001b[A\n",
            "Downloading data:  12% 37.7M/319M [00:00<00:04, 59.0MB/s]\u001b[A\n",
            "Downloading data:  14% 46.1M/319M [00:00<00:04, 64.5MB/s]\u001b[A\n",
            "Downloading data:  17% 54.5M/319M [00:01<00:03, 68.1MB/s]\u001b[A\n",
            "Downloading data:  20% 62.9M/319M [00:01<00:03, 70.3MB/s]\u001b[A\n",
            "Downloading data:  22% 71.3M/319M [00:01<00:03, 71.4MB/s]\u001b[A\n",
            "Downloading data:  25% 79.7M/319M [00:01<00:03, 73.0MB/s]\u001b[A\n",
            "Downloading data:  28% 88.1M/319M [00:01<00:03, 73.1MB/s]\u001b[A\n",
            "Downloading data:  30% 96.5M/319M [00:01<00:04, 55.2MB/s]\u001b[A\n",
            "Downloading data:  33% 105M/319M [00:01<00:03, 58.6MB/s] \u001b[A\n",
            "Downloading data:  35% 113M/319M [00:01<00:03, 62.3MB/s]\u001b[A\n",
            "Downloading data:  38% 122M/319M [00:02<00:02, 66.4MB/s]\u001b[A\n",
            "Downloading data:  41% 130M/319M [00:02<00:02, 67.7MB/s]\u001b[A\n",
            "Downloading data:  43% 138M/319M [00:02<00:02, 69.2MB/s]\u001b[A\n",
            "Downloading data:  46% 147M/319M [00:02<00:02, 72.1MB/s]\u001b[A\n",
            "Downloading data:  49% 155M/319M [00:02<00:02, 72.5MB/s]\u001b[A\n",
            "Downloading data:  51% 164M/319M [00:02<00:02, 67.0MB/s]\u001b[A\n",
            "Downloading data:  54% 172M/319M [00:02<00:02, 68.3MB/s]\u001b[A\n",
            "Downloading data:  56% 180M/319M [00:02<00:02, 64.9MB/s]\u001b[A\n",
            "Downloading data:  59% 189M/319M [00:03<00:01, 66.5MB/s]\u001b[A\n",
            "Downloading data:  62% 197M/319M [00:03<00:01, 69.7MB/s]\u001b[A\n",
            "Downloading data:  64% 206M/319M [00:03<00:01, 70.7MB/s]\u001b[A\n",
            "Downloading data:  67% 214M/319M [00:03<00:01, 70.6MB/s]\u001b[A\n",
            "Downloading data:  70% 222M/319M [00:03<00:02, 44.7MB/s]\u001b[A\n",
            "Downloading data:  72% 231M/319M [00:03<00:01, 51.0MB/s]\u001b[A\n",
            "Downloading data:  75% 239M/319M [00:03<00:01, 56.9MB/s]\u001b[A\n",
            "Downloading data:  77% 247M/319M [00:04<00:01, 61.1MB/s]\u001b[A\n",
            "Downloading data:  80% 256M/319M [00:04<00:00, 63.9MB/s]\u001b[A\n",
            "Downloading data:  83% 264M/319M [00:04<00:00, 66.8MB/s]\u001b[A\n",
            "Downloading data:  85% 273M/319M [00:04<00:00, 70.2MB/s]\u001b[A\n",
            "Downloading data:  88% 281M/319M [00:04<00:00, 72.1MB/s]\u001b[A\n",
            "Downloading data:  91% 289M/319M [00:04<00:00, 68.7MB/s]\u001b[A\n",
            "Downloading data:  93% 298M/319M [00:04<00:00, 66.8MB/s]\u001b[A\n",
            "Downloading data:  96% 306M/319M [00:04<00:00, 70.3MB/s]\u001b[A\n",
            "Downloading data: 100% 319M/319M [00:05<00:00, 61.7MB/s]\n",
            "Downloading data files: 100% 1/1 [00:05<00:00,  5.17s/it]\n",
            "Extracting data files: 100% 1/1 [00:06<00:00,  6.30s/it]\n",
            "Generating train split: 356317 examples [00:02, 163220.15 examples/s]\n",
            "Resolving data files: 100% 1024/1024 [00:11<00:00, 90.39it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 360739.74it/s]\n",
            "Resolving data files: 100% 7168/7168 [00:01<00:00, 6827.54it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 85543.49it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 204756.26it/s]\n",
            "Resolving data files: 100% 59024/59024 [00:00<00:00, 375750.32it/s]\n",
            "Resolving data files: 100% 386/386 [00:00<00:00, 490.61it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 243810.59it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 1051.66it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 347826.96it/s]\n",
            "Resolving data files: 100% 128/128 [00:01<00:00, 92.26it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 226086.61it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 223859.44it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 221253.21it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 371376.33it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 245763.75it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:01<00:00, 803.66it/s]\n",
            "Resolving data files: 100% 2048/2048 [00:00<00:00, 2309.82it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:01<00:00, 1019.81it/s]\n",
            "Resolving data files: 100% 11264/11264 [00:00<00:00, 363896.17it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 132.34it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 197089.17it/s]\n",
            "Resolving data files: 100% 2048/2048 [00:00<00:00, 378244.59it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 281.68it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 239034.24it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 360316.05it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 349810.01it/s]\n",
            "Resolving data files: 100% 64/64 [00:01<00:00, 49.88it/s]\n",
            "Resolving data files: 100% 2048/2048 [00:00<00:00, 373914.36it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 307310.20it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 75509.27it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 376454.32it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 377679.15it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 317863.18it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 228516.48it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 146766.24it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 374680.91it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:01<00:00, 944.58it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 368698.37it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 338826.70it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 307222.27it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 81616.13it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 84760.17it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 335701.68it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 81.49it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 242928.01it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 211034.16it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 359171.04it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 242434.37it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 144242.59it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 297930.58it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 298096.01it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 376487.32it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 356724.86it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 133.92it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 244254.28it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 327660.00it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 364258.10it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 223207.95it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 195938.29it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 364350.81it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 229923.30it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 233765.16it/s]\n",
            "Resolving data files: 100% 4096/4096 [00:01<00:00, 3484.95it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 72198.89it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 66019.54it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 241616.07it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 73908.44it/s]\n",
            "Resolving data files: 100% 512/512 [00:00<00:00, 367091.22it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 330789.22it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 261633.00it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 137026.78it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 324589.43it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 344258.36it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 76216.77it/s]\n",
            "Resolving data files: 100% 256/256 [00:00<00:00, 251638.58it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 303316.90it/s]\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 75360.88it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 229016.07it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 373442.94it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 328637.79it/s]\n",
            "Resolving data files: 100% 3072/3072 [00:00<00:00, 4121.84it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 180886.43it/s]\n",
            "Resolving data files: 100% 128/128 [00:00<00:00, 287404.13it/s]\n",
            "Resolving data files: 100% 32/32 [00:00<00:00, 70014.46it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 228967.23it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 371536.96it/s]\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/40.5M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  10% 4.19M/40.5M [00:00<00:03, 9.22MB/s]\u001b[A\n",
            "Downloading data:  31% 12.6M/40.5M [00:00<00:01, 24.0MB/s]\u001b[A\n",
            "Downloading data:  52% 21.0M/40.5M [00:01<00:00, 21.8MB/s]\u001b[A\n",
            "Downloading data:  73% 29.4M/40.5M [00:01<00:00, 30.1MB/s]\u001b[A\n",
            "Downloading data: 100% 40.5M/40.5M [00:01<00:00, 30.8MB/s]\n",
            "Downloading data files: 100% 1/1 [00:01<00:00,  1.32s/it]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "Generating validation split: 45576 examples [00:00, 188194.59 examples/s]\n",
            "tokenizer_config.json: 100% 776/776 [00:00<00:00, 4.79MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 433MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.25MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 26.8MB/s]\n",
            "config.json: 100% 609/609 [00:00<00:00, 3.29MB/s]\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 82.5MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 31.5M/9.98G [00:00<00:43, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/9.98G [00:00<00:44, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 94.4M/9.98G [00:00<00:44, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 126M/9.98G [00:00<00:45, 215MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 157M/9.98G [00:00<00:45, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 189M/9.98G [00:00<00:45, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 220M/9.98G [00:01<00:45, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 252M/9.98G [00:01<00:44, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 283M/9.98G [00:01<00:44, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 315M/9.98G [00:01<00:44, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 346M/9.98G [00:01<00:44, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 377M/9.98G [00:01<00:44, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 409M/9.98G [00:01<00:45, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 440M/9.98G [00:02<00:44, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 472M/9.98G [00:02<00:44, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 503M/9.98G [00:02<00:44, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 535M/9.98G [00:02<00:43, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 566M/9.98G [00:02<00:43, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 598M/9.98G [00:02<00:43, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 629M/9.98G [00:02<00:43, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 661M/9.98G [00:03<00:43, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 692M/9.98G [00:03<00:43, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 724M/9.98G [00:03<00:42, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 755M/9.98G [00:03<00:42, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 786M/9.98G [00:03<00:42, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 818M/9.98G [00:03<00:43, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 849M/9.98G [00:03<00:44, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 870M/9.98G [00:04<00:45, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 891M/9.98G [00:04<00:45, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 912M/9.98G [00:04<00:46, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 933M/9.98G [00:04<00:46, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 954M/9.98G [00:04<00:48, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 986M/9.98G [00:04<00:45, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.02G/9.98G [00:04<00:44, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.05G/9.98G [00:04<00:43, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.08G/9.98G [00:05<00:42, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.11G/9.98G [00:05<00:42, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.14G/9.98G [00:05<00:41, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.17G/9.98G [00:05<00:41, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.21G/9.98G [00:05<00:40, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.24G/9.98G [00:05<00:40, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.27G/9.98G [00:05<00:40, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.30G/9.98G [00:06<00:40, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.33G/9.98G [00:06<00:39, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.36G/9.98G [00:06<00:39, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.39G/9.98G [00:06<00:39, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.43G/9.98G [00:06<00:39, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.46G/9.98G [00:06<00:39, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.49G/9.98G [00:07<00:39, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.52G/9.98G [00:07<00:39, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.55G/9.98G [00:07<00:39, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.58G/9.98G [00:07<00:38, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.61G/9.98G [00:07<00:38, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.65G/9.98G [00:07<00:38, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.68G/9.98G [00:07<00:38, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.71G/9.98G [00:08<00:38, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.74G/9.98G [00:08<00:38, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.77G/9.98G [00:08<00:38, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.80G/9.98G [00:08<00:38, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.84G/9.98G [00:08<00:37, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.87G/9.98G [00:08<00:37, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.90G/9.98G [00:08<00:37, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.93G/9.98G [00:09<00:38, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.98G [00:09<00:39, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.98G/9.98G [00:09<00:39, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.00G/9.98G [00:09<00:39, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.02G/9.98G [00:09<00:40, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.06G/9.98G [00:09<00:38, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.98G [00:09<00:37, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.12G/9.98G [00:09<00:37, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.15G/9.98G [00:10<00:36, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.18G/9.98G [00:10<00:36, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.21G/9.98G [00:10<00:35, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.24G/9.98G [00:10<00:35, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.28G/9.98G [00:10<00:35, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.31G/9.98G [00:10<00:35, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.34G/9.98G [00:11<00:35, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.37G/9.98G [00:11<00:34, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.40G/9.98G [00:11<00:35, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.43G/9.98G [00:11<00:34, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.46G/9.98G [00:11<00:34, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.50G/9.98G [00:11<00:34, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.53G/9.98G [00:11<00:34, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.56G/9.98G [00:12<00:33, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.59G/9.98G [00:12<00:33, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.62G/9.98G [00:12<00:33, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.65G/9.98G [00:12<00:33, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.68G/9.98G [00:12<00:33, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.72G/9.98G [00:12<00:33, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.75G/9.98G [00:12<00:33, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.78G/9.98G [00:13<00:33, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.81G/9.98G [00:13<00:32, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.84G/9.98G [00:13<00:33, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.87G/9.98G [00:13<00:33, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.90G/9.98G [00:13<00:33, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.94G/9.98G [00:13<00:34, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.96G/9.98G [00:13<00:34, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.98G/9.98G [00:14<00:35, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.00G/9.98G [00:14<00:35, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.02G/9.98G [00:14<00:35, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.04G/9.98G [00:14<00:35, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.06G/9.98G [00:14<00:35, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.08G/9.98G [00:14<00:34, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.10G/9.98G [00:14<00:34, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.14G/9.98G [00:14<00:33, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.17G/9.98G [00:14<00:32, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.20G/9.98G [00:15<00:32, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.23G/9.98G [00:15<00:31, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.26G/9.98G [00:15<00:31, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.29G/9.98G [00:15<00:30, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.32G/9.98G [00:15<00:30, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.36G/9.98G [00:15<00:30, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.39G/9.98G [00:15<00:30, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.42G/9.98G [00:16<00:30, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.45G/9.98G [00:16<00:30, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.48G/9.98G [00:16<00:29, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.51G/9.98G [00:16<00:29, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.54G/9.98G [00:16<00:29, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.58G/9.98G [00:16<00:29, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.61G/9.98G [00:16<00:29, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.64G/9.98G [00:17<00:29, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.67G/9.98G [00:17<00:28, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.70G/9.98G [00:17<00:28, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.73G/9.98G [00:17<00:28, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.76G/9.98G [00:17<00:28, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.80G/9.98G [00:17<00:28, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.83G/9.98G [00:17<00:28, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.86G/9.98G [00:18<00:28, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.89G/9.98G [00:18<00:27, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.92G/9.98G [00:18<00:27, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.95G/9.98G [00:18<00:27, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.98G/9.98G [00:18<00:27, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.02G/9.98G [00:18<00:27, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.05G/9.98G [00:19<00:28, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.07G/9.98G [00:19<00:43, 136MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.09G/9.98G [00:19<00:41, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.11G/9.98G [00:19<00:38, 151MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.14G/9.98G [00:19<00:34, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.17G/9.98G [00:19<00:31, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.20G/9.98G [00:20<00:29, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.24G/9.98G [00:20<00:28, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.27G/9.98G [00:20<00:27, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.30G/9.98G [00:20<00:27, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.33G/9.98G [00:20<00:26, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.36G/9.98G [00:20<00:26, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.39G/9.98G [00:20<00:26, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.42G/9.98G [00:21<00:25, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.46G/9.98G [00:21<00:25, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.49G/9.98G [00:21<00:24, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.52G/9.98G [00:21<00:24, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.55G/9.98G [00:21<00:24, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.58G/9.98G [00:21<00:24, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.61G/9.98G [00:21<00:24, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.65G/9.98G [00:22<00:24, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.68G/9.98G [00:22<00:24, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.71G/9.98G [00:22<00:23, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.74G/9.98G [00:22<00:23, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.77G/9.98G [00:22<00:23, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.80G/9.98G [00:22<00:23, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.83G/9.98G [00:22<00:23, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.87G/9.98G [00:23<00:23, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.90G/9.98G [00:23<00:23, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.93G/9.98G [00:23<00:23, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.96G/9.98G [00:23<00:22, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.99G/9.98G [00:23<00:22, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 5.02G/9.98G [00:23<00:22, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.05G/9.98G [00:23<00:22, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.09G/9.98G [00:24<00:23, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.11G/9.98G [00:24<00:24, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.13G/9.98G [00:24<00:25, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.15G/9.98G [00:24<00:26, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.17G/9.98G [00:24<00:26, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.19G/9.98G [00:24<00:25, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.22G/9.98G [00:24<00:24, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.24G/9.98G [00:24<00:23, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.27G/9.98G [00:25<00:23, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.31G/9.98G [00:25<00:22, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.34G/9.98G [00:25<00:21, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.37G/9.98G [00:25<00:21, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.40G/9.98G [00:25<00:21, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.43G/9.98G [00:25<00:21, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.46G/9.98G [00:25<00:21, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.49G/9.98G [00:26<00:21, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.53G/9.98G [00:26<00:20, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.56G/9.98G [00:26<00:20, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.59G/9.98G [00:26<00:20, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.62G/9.98G [00:26<00:20, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.65G/9.98G [00:26<00:20, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.68G/9.98G [00:26<00:20, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.71G/9.98G [00:27<00:19, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.75G/9.98G [00:27<00:19, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.78G/9.98G [00:27<00:19, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.81G/9.98G [00:27<00:19, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.84G/9.98G [00:27<00:19, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.87G/9.98G [00:27<00:19, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.90G/9.98G [00:27<00:18, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.93G/9.98G [00:28<00:18, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.97G/9.98G [00:28<00:18, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.00G/9.98G [00:28<00:18, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.03G/9.98G [00:28<00:18, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.06G/9.98G [00:28<00:18, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.09G/9.98G [00:28<00:17, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.12G/9.98G [00:29<00:17, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.16G/9.98G [00:29<00:17, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.19G/9.98G [00:29<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.22G/9.98G [00:29<00:18, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.24G/9.98G [00:29<00:18, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.26G/9.98G [00:29<00:18, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.29G/9.98G [00:29<00:17, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.32G/9.98G [00:29<00:17, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.35G/9.98G [00:30<00:17, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.39G/9.98G [00:30<00:16, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.42G/9.98G [00:30<00:16, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.45G/9.98G [00:30<00:16, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.48G/9.98G [00:30<00:16, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.51G/9.98G [00:30<00:16, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.54G/9.98G [00:31<00:15, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.57G/9.98G [00:31<00:15, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.61G/9.98G [00:31<00:15, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.64G/9.98G [00:31<00:15, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.67G/9.98G [00:31<00:15, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.70G/9.98G [00:31<00:15, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.73G/9.98G [00:31<00:14, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.76G/9.98G [00:32<00:14, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.79G/9.98G [00:32<00:14, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.83G/9.98G [00:32<00:14, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.86G/9.98G [00:32<00:14, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.89G/9.98G [00:32<00:14, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.92G/9.98G [00:32<00:14, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.95G/9.98G [00:32<00:13, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.98G/9.98G [00:33<00:13, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 7.01G/9.98G [00:33<00:13, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.05G/9.98G [00:33<00:13, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.08G/9.98G [00:33<00:13, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.11G/9.98G [00:33<00:13, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.14G/9.98G [00:33<00:13, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.17G/9.98G [00:33<00:12, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.20G/9.98G [00:34<00:12, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.24G/9.98G [00:34<00:13, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.27G/9.98G [00:34<00:13, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.29G/9.98G [00:34<00:13, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.32G/9.98G [00:34<00:12, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.34G/9.98G [00:34<00:12, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.36G/9.98G [00:34<00:12, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.39G/9.98G [00:34<00:12, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.41G/9.98G [00:35<00:12, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.44G/9.98G [00:35<00:11, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.48G/9.98G [00:35<00:11, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.51G/9.98G [00:35<00:11, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.54G/9.98G [00:35<00:11, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.57G/9.98G [00:35<00:11, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.60G/9.98G [00:35<00:11, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.63G/9.98G [00:36<00:10, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.67G/9.98G [00:36<00:10, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.70G/9.98G [00:36<00:10, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.73G/9.98G [00:36<00:10, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.76G/9.98G [00:36<00:10, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.79G/9.98G [00:36<00:10, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.82G/9.98G [00:36<00:10, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.85G/9.98G [00:37<00:09, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.89G/9.98G [00:37<00:09, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.92G/9.98G [00:37<00:09, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.95G/9.98G [00:37<00:09, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.98G/9.98G [00:37<00:09, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.01G/9.98G [00:37<00:08, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.04G/9.98G [00:37<00:08, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.07G/9.98G [00:38<00:08, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.11G/9.98G [00:38<00:08, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.14G/9.98G [00:38<00:08, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.17G/9.98G [00:38<00:08, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.20G/9.98G [00:38<00:08, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.23G/9.98G [00:38<00:08, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.26G/9.98G [00:39<00:07, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.29G/9.98G [00:39<00:07, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.33G/9.98G [00:39<00:07, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.36G/9.98G [00:39<00:10, 151MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.39G/9.98G [00:39<00:09, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.42G/9.98G [00:39<00:08, 179MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.45G/9.98G [00:40<00:08, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.48G/9.98G [00:40<00:07, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.51G/9.98G [00:40<00:07, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.55G/9.98G [00:40<00:06, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.58G/9.98G [00:40<00:06, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.61G/9.98G [00:40<00:06, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.64G/9.98G [00:40<00:06, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.67G/9.98G [00:41<00:06, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.70G/9.98G [00:41<00:05, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.73G/9.98G [00:41<00:05, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.77G/9.98G [00:41<00:05, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.80G/9.98G [00:41<00:05, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.83G/9.98G [00:41<00:05, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.85G/9.98G [00:41<00:05, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.87G/9.98G [00:42<00:05, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.89G/9.98G [00:42<00:05, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.91G/9.98G [00:42<00:05, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.93G/9.98G [00:42<00:05, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.95G/9.98G [00:42<00:05, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.98G/9.98G [00:42<00:05, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 9.00G/9.98G [00:42<00:04, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 9.02G/9.98G [00:42<00:04, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.04G/9.98G [00:42<00:04, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.06G/9.98G [00:43<00:04, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.08G/9.98G [00:43<00:04, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.10G/9.98G [00:43<00:04, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.12G/9.98G [00:43<00:04, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.14G/9.98G [00:43<00:04, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.16G/9.98G [00:43<00:04, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.19G/9.98G [00:43<00:04, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.21G/9.98G [00:43<00:04, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.23G/9.98G [00:43<00:03, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.25G/9.98G [00:44<00:03, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.27G/9.98G [00:44<00:03, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.29G/9.98G [00:44<00:03, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.31G/9.98G [00:44<00:03, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.33G/9.98G [00:44<00:03, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.35G/9.98G [00:44<00:03, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.38G/9.98G [00:44<00:02, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.42G/9.98G [00:44<00:02, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.44G/9.98G [00:45<00:02, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.46G/9.98G [00:45<00:02, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.49G/9.98G [00:45<00:02, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.52G/9.98G [00:45<00:02, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.55G/9.98G [00:45<00:01, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.58G/9.98G [00:45<00:01, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.62G/9.98G [00:45<00:01, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.65G/9.98G [00:45<00:01, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.68G/9.98G [00:46<00:01, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.71G/9.98G [00:46<00:01, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.74G/9.98G [00:46<00:01, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.77G/9.98G [00:46<00:00, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.80G/9.98G [00:46<00:00, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.84G/9.98G [00:46<00:00, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.87G/9.98G [00:46<00:00, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.90G/9.98G [00:47<00:00, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.93G/9.98G [00:47<00:00, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:47<00:00, 210MB/s]\n",
            "Downloading shards:  50% 1/2 [00:47<00:47, 47.58s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 31.5M/3.50G [00:00<00:14, 235MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 62.9M/3.50G [00:00<00:15, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 94.4M/3.50G [00:00<00:15, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 126M/3.50G [00:00<00:15, 220MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 157M/3.50G [00:00<00:15, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 189M/3.50G [00:00<00:15, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 220M/3.50G [00:01<00:15, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 252M/3.50G [00:01<00:14, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 283M/3.50G [00:01<00:14, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 315M/3.50G [00:01<00:14, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 346M/3.50G [00:01<00:14, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 377M/3.50G [00:01<00:14, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 409M/3.50G [00:01<00:14, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 430M/3.50G [00:01<00:14, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 451M/3.50G [00:02<00:14, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 482M/3.50G [00:02<00:14, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 514M/3.50G [00:02<00:14, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 545M/3.50G [00:02<00:13, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 577M/3.50G [00:02<00:13, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 608M/3.50G [00:02<00:13, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 640M/3.50G [00:02<00:13, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 671M/3.50G [00:03<00:13, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 703M/3.50G [00:03<00:12, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 734M/3.50G [00:03<00:12, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 765M/3.50G [00:03<00:12, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 797M/3.50G [00:03<00:12, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 828M/3.50G [00:03<00:12, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 860M/3.50G [00:03<00:12, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 891M/3.50G [00:04<00:11, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 923M/3.50G [00:04<00:11, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 954M/3.50G [00:04<00:11, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 986M/3.50G [00:04<00:11, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.02G/3.50G [00:04<00:11, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.05G/3.50G [00:04<00:11, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.08G/3.50G [00:05<00:11, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.11G/3.50G [00:05<00:10, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.14G/3.50G [00:05<00:10, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.17G/3.50G [00:05<00:10, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.21G/3.50G [00:05<00:10, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.24G/3.50G [00:05<00:10, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.27G/3.50G [00:05<00:10, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.30G/3.50G [00:06<00:10, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.33G/3.50G [00:06<00:10, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.36G/3.50G [00:06<00:09, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.39G/3.50G [00:06<00:09, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.43G/3.50G [00:06<00:09, 211MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.46G/3.50G [00:06<00:09, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.48G/3.50G [00:06<00:09, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.50G/3.50G [00:06<00:09, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.52G/3.50G [00:07<00:09, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.54G/3.50G [00:07<00:09, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.56G/3.50G [00:07<00:09, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.58G/3.50G [00:07<00:09, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.61G/3.50G [00:07<00:09, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.65G/3.50G [00:07<00:08, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.68G/3.50G [00:07<00:08, 211MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.71G/3.50G [00:07<00:08, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.74G/3.50G [00:08<00:08, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.77G/3.50G [00:08<00:08, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.80G/3.50G [00:08<00:07, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.84G/3.50G [00:08<00:07, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.87G/3.50G [00:08<00:07, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.90G/3.50G [00:08<00:07, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.93G/3.50G [00:08<00:07, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.96G/3.50G [00:09<00:07, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.99G/3.50G [00:09<00:06, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.02G/3.50G [00:09<00:06, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.06G/3.50G [00:09<00:06, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.09G/3.50G [00:09<00:06, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.12G/3.50G [00:09<00:06, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.15G/3.50G [00:09<00:06, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.18G/3.50G [00:10<00:06, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.21G/3.50G [00:10<00:05, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.24G/3.50G [00:10<00:05, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.28G/3.50G [00:10<00:05, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 2.31G/3.50G [00:10<00:05, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 2.34G/3.50G [00:10<00:05, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 2.37G/3.50G [00:11<00:05, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 2.40G/3.50G [00:11<00:05, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 2.43G/3.50G [00:11<00:04, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 2.46G/3.50G [00:11<00:04, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 2.50G/3.50G [00:11<00:04, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 2.53G/3.50G [00:11<00:06, 148MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 2.56G/3.50G [00:12<00:05, 162MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 2.59G/3.50G [00:12<00:05, 176MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.62G/3.50G [00:12<00:04, 187MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.65G/3.50G [00:12<00:04, 196MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.68G/3.50G [00:12<00:04, 202MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.72G/3.50G [00:12<00:03, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.75G/3.50G [00:12<00:03, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.78G/3.50G [00:13<00:03, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.81G/3.50G [00:13<00:03, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.84G/3.50G [00:13<00:03, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.87G/3.50G [00:13<00:02, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.90G/3.50G [00:13<00:02, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.94G/3.50G [00:13<00:02, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.97G/3.50G [00:14<00:02, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.00G/3.50G [00:14<00:02, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.03G/3.50G [00:14<00:02, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.06G/3.50G [00:14<00:02, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 3.09G/3.50G [00:14<00:01, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 3.12G/3.50G [00:14<00:01, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 3.16G/3.50G [00:14<00:01, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 3.19G/3.50G [00:15<00:01, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 3.22G/3.50G [00:15<00:01, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 3.25G/3.50G [00:15<00:01, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 3.28G/3.50G [00:15<00:01, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 3.31G/3.50G [00:15<00:00, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 3.34G/3.50G [00:15<00:00, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 3.38G/3.50G [00:15<00:00, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 3.41G/3.50G [00:16<00:00, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 3.44G/3.50G [00:16<00:00, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 3.47G/3.50G [00:16<00:00, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:16<00:00, 212MB/s]\n",
            "Downloading shards: 100% 2/2 [01:04<00:00, 32.07s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.05it/s]\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 1.24MB/s]\n",
            "splitting into 1 GPUs\n",
            "100% 100/100 [00:21<00:00,  4.74it/s]\n",
            "saving model gradient at drive/MyDrive/llama-gradients\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python SqueezeLLM-gradients/run.py --output_dir llama-gradients --model_name meta-llama/Llama-2-7b-hf  # single GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we chunk the model at layer level:"
      ],
      "metadata": {
        "id": "ao5qa19F07pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python SqueezeLLM/quantization/chunk_models.py --model meta-llama/Llama-2-7b-hf --output llama-chunks --model_type llama\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAfZwnLGT6hp",
        "outputId": "535e198d-c4dc-47c5-d15a-0226e97c9309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunking the model: meta-llama/Llama-2-7b-hf and storing in drive/MyDrive/llama-chunks\n",
            "config.json: 100% 609/609 [00:00<00:00, 3.19MB/s]\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 89.0MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 31.5M/9.98G [00:00<00:35, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 83.9M/9.98G [00:00<00:26, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 136M/9.98G [00:00<00:24, 394MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 189M/9.98G [00:00<00:23, 408MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 241M/9.98G [00:00<00:23, 416MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 294M/9.98G [00:00<00:23, 419MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 336M/9.98G [00:00<00:23, 418MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 388M/9.98G [00:00<00:22, 420MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 440M/9.98G [00:01<00:22, 422MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 493M/9.98G [00:01<00:22, 423MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 545M/9.98G [00:01<00:22, 422MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 598M/9.98G [00:01<00:22, 417MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 640M/9.98G [00:01<00:22, 415MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 682M/9.98G [00:01<00:22, 414MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 724M/9.98G [00:01<00:22, 410MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 765M/9.98G [00:01<00:23, 396MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 807M/9.98G [00:01<00:23, 385MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 860M/9.98G [00:02<00:22, 398MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 912M/9.98G [00:02<00:22, 409MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 954M/9.98G [00:02<00:21, 411MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.01G/9.98G [00:02<00:21, 416MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.05G/9.98G [00:02<00:21, 417MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.09G/9.98G [00:02<00:21, 413MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.13G/9.98G [00:02<00:22, 395MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.17G/9.98G [00:02<00:22, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.22G/9.98G [00:03<00:24, 360MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.26G/9.98G [00:03<00:24, 358MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.30G/9.98G [00:03<00:24, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.34G/9.98G [00:03<00:25, 343MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.38G/9.98G [00:03<00:24, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.43G/9.98G [00:03<00:24, 344MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.47G/9.98G [00:03<00:24, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.51G/9.98G [00:04<00:31, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.54G/9.98G [00:04<00:35, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.57G/9.98G [00:04<00:35, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.60G/9.98G [00:04<00:34, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.64G/9.98G [00:04<00:33, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.67G/9.98G [00:04<00:31, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.71G/9.98G [00:04<00:28, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.75G/9.98G [00:04<00:26, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.79G/9.98G [00:05<00:26, 307MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.84G/9.98G [00:05<00:25, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.88G/9.98G [00:05<00:23, 339MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.92G/9.98G [00:05<00:23, 341MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.98G [00:05<00:23, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.00G/9.98G [00:05<00:26, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.04G/9.98G [00:05<00:23, 331MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.98G [00:05<00:23, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.13G/9.98G [00:06<00:23, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.17G/9.98G [00:06<00:22, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.21G/9.98G [00:06<00:21, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.25G/9.98G [00:06<00:21, 353MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.30G/9.98G [00:06<00:21, 351MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.34G/9.98G [00:06<00:22, 347MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.38G/9.98G [00:06<00:23, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.42G/9.98G [00:06<00:22, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.46G/9.98G [00:06<00:22, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.51G/9.98G [00:07<00:22, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.55G/9.98G [00:07<00:23, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.59G/9.98G [00:07<00:21, 344MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.63G/9.98G [00:07<00:20, 353MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.67G/9.98G [00:07<00:20, 358MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.72G/9.98G [00:07<00:21, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.76G/9.98G [00:07<00:20, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.80G/9.98G [00:07<00:20, 350MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.84G/9.98G [00:08<00:20, 346MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.88G/9.98G [00:08<00:20, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.93G/9.98G [00:08<00:21, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.97G/9.98G [00:08<00:20, 346MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.01G/9.98G [00:08<00:20, 342MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.05G/9.98G [00:08<00:21, 327MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.09G/9.98G [00:08<00:20, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.14G/9.98G [00:08<00:20, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.18G/9.98G [00:09<00:19, 341MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.22G/9.98G [00:09<00:19, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.26G/9.98G [00:09<00:19, 343MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.98G [00:09<00:19, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.34G/9.98G [00:09<00:19, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.39G/9.98G [00:09<00:19, 331MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.43G/9.98G [00:09<00:18, 345MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.47G/9.98G [00:09<00:18, 346MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.51G/9.98G [00:10<00:18, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.55G/9.98G [00:10<00:17, 358MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.60G/9.98G [00:10<00:17, 355MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.64G/9.98G [00:10<00:18, 351MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.68G/9.98G [00:10<00:18, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.72G/9.98G [00:10<00:19, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.76G/9.98G [00:10<00:18, 327MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.81G/9.98G [00:10<00:17, 350MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.85G/9.98G [00:11<00:17, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.89G/9.98G [00:11<00:17, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.93G/9.98G [00:11<00:17, 351MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.97G/9.98G [00:11<00:19, 305MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.02G/9.98G [00:11<00:24, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.05G/9.98G [00:11<00:26, 228MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.08G/9.98G [00:11<00:24, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.12G/9.98G [00:12<00:21, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.16G/9.98G [00:12<00:19, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.20G/9.98G [00:12<00:17, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.25G/9.98G [00:14<01:59, 47.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.28G/9.98G [00:15<01:41, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.32G/9.98G [00:15<01:14, 76.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.36G/9.98G [00:15<00:56, 98.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.39G/9.98G [00:15<00:47, 117MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.42G/9.98G [00:15<00:40, 137MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.46G/9.98G [00:15<00:35, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.49G/9.98G [00:15<00:32, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.52G/9.98G [00:16<00:31, 172MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.55G/9.98G [00:16<00:31, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.57G/9.98G [00:16<00:31, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.59G/9.98G [00:16<00:30, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.61G/9.98G [00:16<00:30, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.63G/9.98G [00:16<00:30, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.67G/9.98G [00:16<00:27, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.70G/9.98G [00:16<00:25, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.73G/9.98G [00:17<00:24, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.76G/9.98G [00:17<00:21, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.79G/9.98G [00:17<00:20, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.83G/9.98G [00:17<00:18, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.88G/9.98G [00:17<00:16, 307MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.92G/9.98G [00:17<00:16, 308MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.95G/9.98G [00:17<00:16, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.99G/9.98G [00:17<00:15, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 5.03G/9.98G [00:18<00:16, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.08G/9.98G [00:18<00:14, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.12G/9.98G [00:18<00:15, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.16G/9.98G [00:18<00:13, 347MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.20G/9.98G [00:18<00:13, 350MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.24G/9.98G [00:18<00:13, 346MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.28G/9.98G [00:18<00:12, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.33G/9.98G [00:18<00:12, 375MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.37G/9.98G [00:18<00:12, 376MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.41G/9.98G [00:19<00:12, 370MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.45G/9.98G [00:19<00:11, 377MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.49G/9.98G [00:19<00:11, 375MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.54G/9.98G [00:19<00:11, 378MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.58G/9.98G [00:19<00:11, 381MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.62G/9.98G [00:19<00:11, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.66G/9.98G [00:19<00:11, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.70G/9.98G [00:19<00:11, 372MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.75G/9.98G [00:19<00:11, 378MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.79G/9.98G [00:20<00:10, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.83G/9.98G [00:20<00:10, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.88G/9.98G [00:20<00:10, 400MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.93G/9.98G [00:20<00:09, 409MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.99G/9.98G [00:20<00:09, 413MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.04G/9.98G [00:20<00:09, 416MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.09G/9.98G [00:20<00:09, 419MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.13G/9.98G [00:20<00:09, 415MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.18G/9.98G [00:21<00:09, 412MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.23G/9.98G [00:21<00:09, 416MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.27G/9.98G [00:21<00:10, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.31G/9.98G [00:21<00:12, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.35G/9.98G [00:21<00:13, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.39G/9.98G [00:21<00:14, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.42G/9.98G [00:21<00:13, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.45G/9.98G [00:22<00:13, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.48G/9.98G [00:22<00:13, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.51G/9.98G [00:22<00:12, 270MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.54G/9.98G [00:22<00:12, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.57G/9.98G [00:22<00:12, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.61G/9.98G [00:22<00:12, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.64G/9.98G [00:22<00:12, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.67G/9.98G [00:22<00:12, 259MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.70G/9.98G [00:23<00:13, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.73G/9.98G [00:23<00:12, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.77G/9.98G [00:23<00:11, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.82G/9.98G [00:23<00:09, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.86G/9.98G [00:23<00:09, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.90G/9.98G [00:25<00:46, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.93G/9.98G [00:25<00:37, 81.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.96G/9.98G [00:25<00:29, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.99G/9.98G [00:25<00:24, 123MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.04G/9.98G [00:25<00:18, 159MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.08G/9.98G [00:25<00:14, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.12G/9.98G [00:25<00:12, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.16G/9.98G [00:25<00:10, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.20G/9.98G [00:26<00:09, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.25G/9.98G [00:26<00:08, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.29G/9.98G [00:26<00:08, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.33G/9.98G [00:26<00:10, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.36G/9.98G [00:26<00:10, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.39G/9.98G [00:26<00:11, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.42G/9.98G [00:27<00:11, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.46G/9.98G [00:27<00:12, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.49G/9.98G [00:27<00:12, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.51G/9.98G [00:27<00:12, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.54G/9.98G [00:27<00:11, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.57G/9.98G [00:27<00:11, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.60G/9.98G [00:27<00:10, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.64G/9.98G [00:28<00:09, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.68G/9.98G [00:28<00:08, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.71G/9.98G [00:28<00:08, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.74G/9.98G [00:28<00:07, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.77G/9.98G [00:28<00:11, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.81G/9.98G [00:28<00:09, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.85G/9.98G [00:28<00:07, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.90G/9.98G [00:28<00:06, 298MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.94G/9.98G [00:29<00:06, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.98G/9.98G [00:29<00:06, 312MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.02G/9.98G [00:29<00:06, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.06G/9.98G [00:29<00:05, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.11G/9.98G [00:29<00:05, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.15G/9.98G [00:29<00:05, 346MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.19G/9.98G [00:29<00:05, 352MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.23G/9.98G [00:29<00:05, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.27G/9.98G [00:30<00:04, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.32G/9.98G [00:30<00:04, 360MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.36G/9.98G [00:30<00:04, 375MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.41G/9.98G [00:30<00:04, 391MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.46G/9.98G [00:30<00:03, 402MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.50G/9.98G [00:30<00:03, 407MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.56G/9.98G [00:30<00:03, 411MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.60G/9.98G [00:30<00:03, 413MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.64G/9.98G [00:30<00:03, 400MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.68G/9.98G [00:31<00:03, 394MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.73G/9.98G [00:31<00:03, 402MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.78G/9.98G [00:31<00:02, 406MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.82G/9.98G [00:31<00:02, 407MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.86G/9.98G [00:31<00:02, 408MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.90G/9.98G [00:31<00:02, 410MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.94G/9.98G [00:31<00:02, 408MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.99G/9.98G [00:31<00:02, 342MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 9.03G/9.98G [00:32<00:03, 308MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.07G/9.98G [00:32<00:03, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.10G/9.98G [00:32<00:03, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.13G/9.98G [00:32<00:03, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.16G/9.98G [00:32<00:02, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.21G/9.98G [00:32<00:02, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.25G/9.98G [00:32<00:02, 314MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.29G/9.98G [00:32<00:02, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.33G/9.98G [00:33<00:01, 340MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.37G/9.98G [00:33<00:01, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.42G/9.98G [00:33<00:01, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.46G/9.98G [00:33<00:01, 367MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.50G/9.98G [00:33<00:01, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.54G/9.98G [00:33<00:01, 375MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.58G/9.98G [00:33<00:01, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.63G/9.98G [00:33<00:00, 377MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.67G/9.98G [00:33<00:00, 374MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.71G/9.98G [00:34<00:00, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.75G/9.98G [00:34<00:00, 385MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.79G/9.98G [00:34<00:00, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.84G/9.98G [00:34<00:00, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.88G/9.98G [00:34<00:00, 374MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.92G/9.98G [00:34<00:00, 376MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:34<00:00, 287MB/s]\n",
            "Downloading shards:  50% 1/2 [00:34<00:34, 34.93s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 41.9M/3.50G [00:00<00:08, 396MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 83.9M/3.50G [00:00<00:08, 389MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 126M/3.50G [00:00<00:08, 401MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 178M/3.50G [00:00<00:08, 415MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 231M/3.50G [00:00<00:07, 423MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 283M/3.50G [00:00<00:07, 426MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 336M/3.50G [00:00<00:07, 428MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 388M/3.50G [00:00<00:07, 430MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 440M/3.50G [00:01<00:07, 417MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 482M/3.50G [00:01<00:07, 381MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 524M/3.50G [00:01<00:08, 352MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 566M/3.50G [00:01<00:09, 321MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 608M/3.50G [00:01<00:09, 307MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 640M/3.50G [00:01<00:09, 295MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 671M/3.50G [00:01<00:09, 284MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 703M/3.50G [00:02<00:10, 278MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 734M/3.50G [00:02<00:10, 270MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 765M/3.50G [00:02<00:10, 267MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 797M/3.50G [00:02<00:10, 264MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 828M/3.50G [00:02<00:09, 271MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 860M/3.50G [00:02<00:09, 269MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 891M/3.50G [00:02<00:09, 265MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 923M/3.50G [00:02<00:09, 269MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 965M/3.50G [00:02<00:09, 279MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.01G/3.50G [00:03<00:08, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.05G/3.50G [00:03<00:07, 330MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.09G/3.50G [00:03<00:07, 344MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.13G/3.50G [00:03<00:06, 348MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.17G/3.50G [00:03<00:06, 356MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.22G/3.50G [00:03<00:06, 361MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.26G/3.50G [00:03<00:09, 235MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.30G/3.50G [00:04<00:08, 264MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.34G/3.50G [00:04<00:07, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.39G/3.50G [00:04<00:06, 328MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.44G/3.50G [00:04<00:06, 341MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.49G/3.50G [00:04<00:05, 365MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.53G/3.50G [00:04<00:05, 353MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.57G/3.50G [00:04<00:05, 344MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.61G/3.50G [00:04<00:05, 330MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.66G/3.50G [00:05<00:05, 310MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.70G/3.50G [00:05<00:05, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.73G/3.50G [00:05<00:05, 308MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.76G/3.50G [00:05<00:05, 308MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.79G/3.50G [00:05<00:05, 308MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.82G/3.50G [00:05<00:05, 302MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.86G/3.50G [00:05<00:05, 298MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.90G/3.50G [00:05<00:05, 308MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.93G/3.50G [00:06<00:05, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.96G/3.50G [00:06<00:05, 304MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.99G/3.50G [00:06<00:05, 289MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.02G/3.50G [00:06<00:05, 293MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.07G/3.50G [00:06<00:04, 305MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.11G/3.50G [00:06<00:04, 326MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.15G/3.50G [00:06<00:03, 347MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.19G/3.50G [00:06<00:03, 363MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.23G/3.50G [00:06<00:03, 368MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.28G/3.50G [00:07<00:03, 362MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 2.32G/3.50G [00:07<00:03, 370MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 2.36G/3.50G [00:07<00:03, 349MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 2.40G/3.50G [00:07<00:03, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 2.44G/3.50G [00:07<00:02, 359MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 2.49G/3.50G [00:07<00:02, 353MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 2.53G/3.50G [00:07<00:02, 365MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 2.57G/3.50G [00:07<00:02, 374MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.61G/3.50G [00:07<00:02, 381MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.65G/3.50G [00:08<00:02, 377MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.69G/3.50G [00:08<00:02, 375MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.74G/3.50G [00:08<00:02, 380MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.78G/3.50G [00:08<00:01, 371MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.82G/3.50G [00:08<00:01, 378MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.86G/3.50G [00:08<00:01, 375MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.90G/3.50G [00:08<00:01, 379MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.95G/3.50G [00:08<00:01, 381MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.99G/3.50G [00:08<00:01, 385MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.03G/3.50G [00:09<00:01, 356MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 3.07G/3.50G [00:09<00:01, 349MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 3.11G/3.50G [00:09<00:01, 345MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 3.16G/3.50G [00:09<00:00, 351MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 3.20G/3.50G [00:09<00:00, 352MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 3.24G/3.50G [00:09<00:00, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 3.28G/3.50G [00:09<00:00, 368MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 3.32G/3.50G [00:09<00:00, 367MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 3.37G/3.50G [00:09<00:00, 370MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 3.41G/3.50G [00:10<00:00, 355MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 3.45G/3.50G [00:10<00:00, 320MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:10<00:00, 336MB/s]\n",
            "Downloading shards: 100% 2/2 [00:45<00:00, 22.74s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.43it/s]\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 934kB/s]\n",
            "32it [00:43,  1.35s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do the same for the gradients computer earlier:"
      ],
      "metadata": {
        "id": "UfFyHgH-1DYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python SqueezeLLM/quantization/chunk_models.py --model llama-gradients --output llama-gradients-chunks --model_type llama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNTORwQRadJp",
        "outputId": "fdec41df-6b5b-4d04-a54e-d0d6c8a8a6a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunking the model: drive/MyDrive/llama-gradients and storing in drive/MyDrive/llama-gradients-chunks\n",
            "Loading checkpoint shards: 100% 3/3 [02:47<00:00, 55.68s/it]\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at drive/MyDrive/llama-gradients and are newly initialized: ['model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "32it [00:41,  1.29s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following searches for the outliers to keep in the sparse matrix:"
      ],
      "metadata": {
        "id": "OjcgIW_D1Hkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python SqueezeLLM/quantization/generate_outlier_config.py --model drive/MyDrive/llama-chunks --range 1.8 --output drive/MyDrive/llama-chunks-outlierconfig/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg-Wd-S2kCEA",
        "outputId": "8052ece2-f9d1-4e67-e53c-36dd77f0f9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type : llama\n",
            "\r  0% 0/32 [00:00<?, ?it/s]0 q % outlier: 7.062%\n",
            "0 k % outlier: 5.751%\n",
            "0 v % outlier: 2.658%\n",
            "0 o % outlier: 3.800%\n",
            "0 gate % outlier: 0.333%\n",
            "0 up % outlier: 0.266%\n",
            "0 down % outlier: 0.289%\n",
            "  3% 1/32 [00:21<11:06, 21.49s/it]1 q % outlier: 6.485%\n",
            "1 k % outlier: 5.458%\n",
            "1 v % outlier: 2.533%\n",
            "1 o % outlier: 4.695%\n",
            "1 gate % outlier: 0.281%\n",
            "1 up % outlier: 0.227%\n",
            "1 down % outlier: 0.229%\n",
            "  6% 2/32 [00:41<10:10, 20.36s/it]2 q % outlier: 2.780%\n",
            "2 k % outlier: 3.912%\n",
            "2 v % outlier: 0.324%\n",
            "2 o % outlier: 0.332%\n",
            "2 gate % outlier: 0.230%\n",
            "2 up % outlier: 0.216%\n",
            "2 down % outlier: 0.209%\n",
            "  9% 3/32 [00:59<09:28, 19.62s/it]3 q % outlier: 1.796%\n",
            "3 k % outlier: 2.264%\n",
            "3 v % outlier: 0.372%\n",
            "3 o % outlier: 0.366%\n",
            "3 gate % outlier: 0.244%\n",
            "3 up % outlier: 0.216%\n",
            "3 down % outlier: 0.210%\n",
            " 12% 4/32 [01:18<09:00, 19.32s/it]4 q % outlier: 1.377%\n",
            "4 k % outlier: 1.795%\n",
            "4 v % outlier: 0.291%\n",
            "4 o % outlier: 0.269%\n",
            "4 gate % outlier: 0.303%\n",
            "4 up % outlier: 0.224%\n",
            "4 down % outlier: 0.229%\n",
            " 16% 5/32 [01:37<08:39, 19.24s/it]5 q % outlier: 1.145%\n",
            "5 k % outlier: 1.525%\n",
            "5 v % outlier: 0.342%\n",
            "5 o % outlier: 0.295%\n",
            "5 gate % outlier: 0.352%\n",
            "5 up % outlier: 0.227%\n",
            "5 down % outlier: 0.232%\n",
            " 19% 6/32 [01:56<08:18, 19.18s/it]6 q % outlier: 1.628%\n",
            "6 k % outlier: 1.893%\n",
            "6 v % outlier: 0.437%\n",
            "6 o % outlier: 0.366%\n",
            "6 gate % outlier: 0.385%\n",
            "6 up % outlier: 0.228%\n",
            "6 down % outlier: 0.228%\n",
            " 22% 7/32 [02:16<07:59, 19.20s/it]7 q % outlier: 1.630%\n",
            "7 k % outlier: 1.774%\n",
            "7 v % outlier: 0.465%\n",
            "7 o % outlier: 0.415%\n",
            "7 gate % outlier: 0.409%\n",
            "7 up % outlier: 0.228%\n",
            "7 down % outlier: 0.226%\n",
            " 25% 8/32 [02:34<07:33, 18.91s/it]8 q % outlier: 1.474%\n",
            "8 k % outlier: 1.617%\n",
            "8 v % outlier: 0.359%\n",
            "8 o % outlier: 0.310%\n",
            "8 gate % outlier: 0.417%\n",
            "8 up % outlier: 0.228%\n",
            "8 down % outlier: 0.243%\n",
            " 28% 9/32 [02:53<07:13, 18.84s/it]9 q % outlier: 1.101%\n",
            "9 k % outlier: 1.357%\n",
            "9 v % outlier: 0.383%\n",
            "9 o % outlier: 0.315%\n",
            "9 gate % outlier: 0.448%\n",
            "9 up % outlier: 0.238%\n",
            "9 down % outlier: 0.232%\n",
            " 31% 10/32 [03:13<07:03, 19.23s/it]10 q % outlier: 1.251%\n",
            "10 k % outlier: 1.616%\n",
            "10 v % outlier: 0.322%\n",
            "10 o % outlier: 0.287%\n",
            "10 gate % outlier: 0.525%\n",
            "10 up % outlier: 0.248%\n",
            "10 down % outlier: 0.246%\n",
            " 34% 11/32 [03:32<06:42, 19.18s/it]11 q % outlier: 1.817%\n",
            "11 k % outlier: 1.909%\n",
            "11 v % outlier: 0.388%\n",
            "11 o % outlier: 0.354%\n",
            "11 gate % outlier: 0.476%\n",
            "11 up % outlier: 0.245%\n",
            "11 down % outlier: 0.237%\n",
            " 38% 12/32 [03:51<06:24, 19.23s/it]12 q % outlier: 1.214%\n",
            "12 k % outlier: 1.492%\n",
            "12 v % outlier: 0.401%\n",
            "12 o % outlier: 0.342%\n",
            "12 gate % outlier: 0.450%\n",
            "12 up % outlier: 0.235%\n",
            "12 down % outlier: 0.247%\n",
            " 41% 13/32 [04:11<06:08, 19.42s/it]13 q % outlier: 1.173%\n",
            "13 k % outlier: 1.321%\n",
            "13 v % outlier: 0.377%\n",
            "13 o % outlier: 0.324%\n",
            "13 gate % outlier: 0.453%\n",
            "13 up % outlier: 0.242%\n",
            "13 down % outlier: 0.236%\n",
            " 44% 14/32 [04:30<05:47, 19.30s/it]14 q % outlier: 1.232%\n",
            "14 k % outlier: 1.395%\n",
            "14 v % outlier: 0.336%\n",
            "14 o % outlier: 0.291%\n",
            "14 gate % outlier: 0.426%\n",
            "14 up % outlier: 0.242%\n",
            "14 down % outlier: 0.236%\n",
            " 47% 15/32 [04:49<05:27, 19.25s/it]15 q % outlier: 1.003%\n",
            "15 k % outlier: 1.234%\n",
            "15 v % outlier: 0.309%\n",
            "15 o % outlier: 0.290%\n",
            "15 gate % outlier: 0.456%\n",
            "15 up % outlier: 0.249%\n",
            "15 down % outlier: 0.238%\n",
            " 50% 16/32 [05:10<05:14, 19.67s/it]16 q % outlier: 1.168%\n",
            "16 k % outlier: 1.401%\n",
            "16 v % outlier: 0.317%\n",
            "16 o % outlier: 0.281%\n",
            "16 gate % outlier: 0.425%\n",
            "16 up % outlier: 0.240%\n",
            "16 down % outlier: 0.229%\n",
            " 53% 17/32 [05:30<04:55, 19.73s/it]17 q % outlier: 0.914%\n",
            "17 k % outlier: 1.127%\n",
            "17 v % outlier: 0.285%\n",
            "17 o % outlier: 0.277%\n",
            "17 gate % outlier: 0.416%\n",
            "17 up % outlier: 0.224%\n",
            "17 down % outlier: 0.220%\n",
            " 56% 18/32 [05:48<04:32, 19.47s/it]18 q % outlier: 0.800%\n",
            "18 k % outlier: 0.966%\n",
            "18 v % outlier: 0.301%\n",
            "18 o % outlier: 0.269%\n",
            "18 gate % outlier: 0.394%\n",
            "18 up % outlier: 0.223%\n",
            "18 down % outlier: 0.214%\n",
            " 59% 19/32 [06:07<04:11, 19.32s/it]19 q % outlier: 0.909%\n",
            "19 k % outlier: 1.106%\n",
            "19 v % outlier: 0.321%\n",
            "19 o % outlier: 0.282%\n",
            "19 gate % outlier: 0.361%\n",
            "19 up % outlier: 0.220%\n",
            "19 down % outlier: 0.218%\n",
            " 62% 20/32 [06:26<03:49, 19.16s/it]20 q % outlier: 1.020%\n",
            "20 k % outlier: 1.214%\n",
            "20 v % outlier: 0.249%\n",
            "20 o % outlier: 0.246%\n",
            "20 gate % outlier: 0.328%\n",
            "20 up % outlier: 0.215%\n",
            "20 down % outlier: 0.214%\n",
            " 66% 21/32 [06:45<03:29, 19.03s/it]21 q % outlier: 1.013%\n",
            "21 k % outlier: 1.204%\n",
            "21 v % outlier: 0.265%\n",
            "21 o % outlier: 0.235%\n",
            "21 gate % outlier: 0.313%\n",
            "21 up % outlier: 0.210%\n",
            "21 down % outlier: 0.203%\n",
            " 69% 22/32 [07:04<03:11, 19.12s/it]22 q % outlier: 0.884%\n",
            "22 k % outlier: 1.010%\n",
            "22 v % outlier: 0.270%\n",
            "22 o % outlier: 0.264%\n",
            "22 gate % outlier: 0.298%\n",
            "22 up % outlier: 0.206%\n",
            "22 down % outlier: 0.203%\n",
            " 72% 23/32 [07:24<02:53, 19.32s/it]23 q % outlier: 0.737%\n",
            "23 k % outlier: 0.817%\n",
            "23 v % outlier: 0.275%\n",
            "23 o % outlier: 0.267%\n",
            "23 gate % outlier: 0.278%\n",
            "23 up % outlier: 0.205%\n",
            "23 down % outlier: 0.205%\n",
            " 75% 24/32 [07:43<02:34, 19.29s/it]24 q % outlier: 1.180%\n",
            "24 k % outlier: 1.328%\n",
            "24 v % outlier: 0.267%\n",
            "24 o % outlier: 0.247%\n",
            "24 gate % outlier: 0.279%\n",
            "24 up % outlier: 0.203%\n",
            "24 down % outlier: 0.201%\n",
            " 78% 25/32 [08:02<02:14, 19.27s/it]25 q % outlier: 0.857%\n",
            "25 k % outlier: 0.898%\n",
            "25 v % outlier: 0.263%\n",
            "25 o % outlier: 0.242%\n",
            "25 gate % outlier: 0.295%\n",
            "25 up % outlier: 0.203%\n",
            "25 down % outlier: 0.205%\n",
            " 81% 26/32 [08:22<01:55, 19.26s/it]26 q % outlier: 0.953%\n",
            "26 k % outlier: 1.040%\n",
            "26 v % outlier: 0.322%\n",
            "26 o % outlier: 0.394%\n",
            "26 gate % outlier: 0.352%\n",
            "26 up % outlier: 0.213%\n",
            "26 down % outlier: 0.214%\n",
            " 84% 27/32 [08:40<01:35, 19.03s/it]27 q % outlier: 0.574%\n",
            "27 k % outlier: 0.604%\n",
            "27 v % outlier: 0.262%\n",
            "27 o % outlier: 0.269%\n",
            "27 gate % outlier: 0.390%\n",
            "27 up % outlier: 0.207%\n",
            "27 down % outlier: 0.219%\n",
            " 88% 28/32 [08:59<01:15, 18.84s/it]28 q % outlier: 0.709%\n",
            "28 k % outlier: 0.757%\n",
            "28 v % outlier: 0.315%\n",
            "28 o % outlier: 0.296%\n",
            "28 gate % outlier: 0.431%\n",
            "28 up % outlier: 0.215%\n",
            "28 down % outlier: 0.221%\n",
            " 91% 29/32 [09:17<00:56, 18.84s/it]29 q % outlier: 0.893%\n",
            "29 k % outlier: 0.990%\n",
            "29 v % outlier: 0.278%\n",
            "29 o % outlier: 0.272%\n",
            "29 gate % outlier: 0.493%\n",
            "29 up % outlier: 0.223%\n",
            "29 down % outlier: 0.242%\n",
            " 94% 30/32 [09:37<00:38, 19.18s/it]30 q % outlier: 0.635%\n",
            "30 k % outlier: 0.651%\n",
            "30 v % outlier: 0.260%\n",
            "30 o % outlier: 0.260%\n",
            "30 gate % outlier: 0.551%\n",
            "30 up % outlier: 0.250%\n",
            "30 down % outlier: 0.298%\n",
            " 97% 31/32 [09:57<00:19, 19.38s/it]31 q % outlier: 0.702%\n",
            "31 k % outlier: 0.900%\n",
            "31 v % outlier: 0.344%\n",
            "31 o % outlier: 0.376%\n",
            "31 gate % outlier: 0.542%\n",
            "31 up % outlier: 0.314%\n",
            "31 down % outlier: 0.384%\n",
            "100% 32/32 [10:16<00:00, 19.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For 4-bit quantization"
      ],
      "metadata": {
        "id": "C_HUnEgi1nS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can perform the non-uniform quantization with kmeans. You only need a CPU for this step. If you use Google Colab, this will take several hours."
      ],
      "metadata": {
        "id": "g0VwcjkI1QKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python SqueezeLLM/quantization/nuq.py --bit 4 --model_type llama --model drive/MyDrive/llama-chunks --gradient drive/MyDrive/llama-gradients-chunks --output drive/MyDrive/llama-LUT/ --outlier_config drive/MyDrive/llama-chunks-outlierconfig/outlier_config_o0.53.json --sensitivity 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xstvtKNak3R5",
        "outputId": "2103c0b4-9a70-40f0-d0f4-8e435251e003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running outlier mode with outlier folder threshold 0.53 and sensitivity 0.05\n",
            "Quantizing layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
            "Quantizing layer 0\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04984307165590593\n",
            "removing outliers by threshold\n",
            "p outlier: 1.7908066665570352\n",
            "100% 7/7 [09:58<00:00, 85.56s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l0.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l0.pkl\n",
            "Quantizing layer 1\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049876672616276714\n",
            "removing outliers by threshold\n",
            "p outlier: 1.7513487623145543\n",
            "100% 7/7 [09:40<00:00, 82.97s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l1.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l1.pkl\n",
            "Quantizing layer 2\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049625159545265946\n",
            "removing outliers by threshold\n",
            "p outlier: 0.7607544024373583\n",
            " 57% 4/7 [03:03<02:18, 46.18s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            " 71% 5/7 [05:11<02:31, 75.88s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [08:36<00:00, 73.78s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l2.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l2.pkl\n",
            "Quantizing layer 3\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049734362666471015\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5478799651941487\n",
            "100% 7/7 [08:38<00:00, 74.13s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l3.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l3.pkl\n",
            "Quantizing layer 4\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049656289846785946\n",
            "removing outliers by threshold\n",
            "p outlier: 0.47762727490360873\n",
            " 57% 4/7 [03:37<02:46, 55.34s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [10:16<00:00, 88.04s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l4.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l4.pkl\n",
            "Quantizing layer 5\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04972596242637831\n",
            "removing outliers by threshold\n",
            "p outlier: 0.45382494753506514\n",
            "100% 7/7 [10:18<00:00, 88.30s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l5.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l5.pkl\n",
            "Quantizing layer 6\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04970718541911229\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5449309867898418\n",
            "100% 7/7 [10:13<00:00, 87.67s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l6.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l6.pkl\n",
            "Quantizing layer 7\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04970076170610023\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5464573598278619\n",
            "100% 7/7 [10:21<00:00, 88.75s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l7.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l7.pkl\n",
            "Quantizing layer 8\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04979168195180943\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5102345362846098\n",
            "100% 7/7 [09:49<00:00, 84.26s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l8.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l8.pkl\n",
            "Quantizing layer 9\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.0497274448216888\n",
            "removing outliers by threshold\n",
            "p outlier: 0.46529473418398837\n",
            " 71% 5/7 [06:08<02:51, 85.73s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [10:12<00:00, 87.55s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l9.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l9.pkl\n",
            "Quantizing layer 10\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04977191668100307\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5154184726853445\n",
            "100% 7/7 [10:17<00:00, 88.16s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l10.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l10.pkl\n",
            "Quantizing layer 11\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04959649990259674\n",
            "removing outliers by threshold\n",
            "p outlier: 0.584246581082517\n",
            "100% 7/7 [09:34<00:00, 82.12s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l11.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l11.pkl\n",
            "Quantizing layer 12\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04969680865193896\n",
            "removing outliers by threshold\n",
            "p outlier: 0.492712623714783\n",
            "100% 7/7 [08:57<00:00, 76.75s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l12.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l12.pkl\n",
            "Quantizing layer 13\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049604900142689444\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4779143654620709\n",
            " 71% 5/7 [05:28<02:35, 77.86s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [09:33<00:00, 81.96s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l13.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l13.pkl\n",
            "Quantizing layer 14\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049513485765210086\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4724715040137731\n",
            "100% 7/7 [11:05<00:00, 95.02s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l14.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l14.pkl\n",
            "Quantizing layer 15\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04983220075696243\n",
            "removing outliers by threshold\n",
            "p outlier: 0.44420074304768453\n",
            "100% 7/7 [10:34<00:00, 90.60s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l15.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l15.pkl\n",
            "Quantizing layer 16\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.0497704342856926\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4605085738582314\n",
            " 71% 5/7 [05:52<02:50, 85.23s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [09:56<00:00, 85.27s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l16.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l16.pkl\n",
            "Quantizing layer 17\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04961379451455229\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4064831708997025\n",
            " 71% 5/7 [06:05<02:51, 85.94s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [10:06<00:00, 86.69s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l17.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l17.pkl\n",
            "Quantizing layer 18\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049541651276109135\n",
            "removing outliers by threshold\n",
            "p outlier: 0.37808393567337273\n",
            " 71% 5/7 [05:52<02:50, 85.34s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [09:55<00:00, 85.04s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l18.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l18.pkl\n",
            "Quantizing layer 19\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04967457272228181\n",
            "removing outliers by threshold\n",
            "p outlier: 0.39431764671839575\n",
            " 71% 5/7 [06:16<02:53, 86.86s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [10:16<00:00, 88.06s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l19.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l19.pkl\n",
            "Quantizing layer 20\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04963405391712881\n",
            "removing outliers by threshold\n",
            "p outlier: 0.39396137771211137\n",
            "100% 7/7 [10:14<00:00, 87.81s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l20.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l20.pkl\n",
            "Quantizing layer 21\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049704220628491336\n",
            "removing outliers by threshold\n",
            "p outlier: 0.38834655840779836\n",
            " 71% 5/7 [05:27<02:30, 75.42s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [08:53<00:00, 76.14s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l21.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l21.pkl\n",
            "Quantizing layer 22\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04957278157762913\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3588127966371843\n",
            " 71% 5/7 [05:03<02:26, 73.10s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [08:26<00:00, 72.39s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l22.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l22.pkl\n",
            "Quantizing layer 23\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.0496676548774996\n",
            "removing outliers by threshold\n",
            "p outlier: 0.32626581933214255\n",
            "100% 7/7 [08:57<00:00, 76.85s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l23.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l23.pkl\n",
            "Quantizing layer 24\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049805517641373864\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4045170205862411\n",
            " 57% 4/7 [03:11<02:24, 48.10s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [08:50<00:00, 75.75s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l24.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l24.pkl\n",
            "Quantizing layer 25\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049596994034366904\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3431488195231541\n",
            "100% 7/7 [10:09<00:00, 87.13s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l25.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l25.pkl\n",
            "Quantizing layer 26\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04951941534645199\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3972582248826101\n",
            "100% 7/7 [10:46<00:00, 92.33s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l26.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l26.pkl\n",
            "Quantizing layer 27\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04972596242637831\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3231552598389937\n",
            "  0% 0/7 [00:00<?, ?it/s]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [10:41<00:00, 91.58s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l27.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l27.pkl\n",
            "Quantizing layer 28\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049877660879817036\n",
            "removing outliers by threshold\n",
            "p outlier: 0.36499586747717977\n",
            "  0% 0/7 [00:00<?, ?it/s]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            " 14% 1/7 [00:57<05:47, 57.98s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            " 57% 4/7 [03:43<02:46, 55.58s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [10:35<00:00, 90.79s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l28.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l28.pkl\n",
            "Quantizing layer 29\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04963405391712881\n",
            "removing outliers by threshold\n",
            "p outlier: 0.413955431528042\n",
            "100% 7/7 [10:17<00:00, 88.18s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l29.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l29.pkl\n",
            "Quantizing layer 30\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.0498361538111237\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3970655134922482\n",
            " 57% 4/7 [03:42<02:44, 54.81s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            " 71% 5/7 [06:08<02:55, 87.99s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [10:14<00:00, 87.80s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l30.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l30.pkl\n",
            "Quantizing layer 31\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04955746349275421\n",
            "removing outliers by threshold\n",
            "p outlier: 0.47061752161213766\n",
            " 57% 4/7 [03:57<02:49, 56.42s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [10:23<00:00, 89.06s/it] \n",
            "Saving layer lut to drive/MyDrive/llama-LUT//lut/l31.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT//outliers/l31.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we repack the model:"
      ],
      "metadata": {
        "id": "cFrhklx01c-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python SqueezeLLM/quantization/pack.py --model meta-llama/Llama-2-7b-hf --wbits 4 --folder drive/MyDrive/llama-LUT/ --save drive/MyDrive/llama-suqeezed --include_sparse --balance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sapfXt6l0b1",
        "outputId": "9b3773e8-15c4-4da7-d9b0-472c5a45314b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json: 100% 609/609 [00:00<00:00, 2.20MB/s]\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 6.62MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 31.5M/9.98G [00:00<00:35, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/9.98G [00:00<00:29, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 115M/9.98G [00:00<00:27, 357MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 157M/9.98G [00:00<00:26, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 199M/9.98G [00:00<00:26, 371MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 241M/9.98G [00:00<00:25, 375MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 283M/9.98G [00:00<00:25, 377MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 325M/9.98G [00:00<00:25, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 367M/9.98G [00:00<00:25, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 409M/9.98G [00:01<00:24, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 451M/9.98G [00:01<00:24, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 493M/9.98G [00:01<00:24, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 535M/9.98G [00:01<00:24, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 577M/9.98G [00:01<00:25, 374MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 619M/9.98G [00:01<00:25, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 661M/9.98G [00:01<00:24, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 703M/9.98G [00:01<00:24, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 744M/9.98G [00:01<00:24, 375MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 786M/9.98G [00:02<00:24, 368MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 828M/9.98G [00:02<00:25, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 870M/9.98G [00:02<00:25, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 912M/9.98G [00:02<00:25, 352MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 954M/9.98G [00:02<00:24, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 996M/9.98G [00:02<00:24, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.04G/9.98G [00:02<00:24, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.08G/9.98G [00:05<02:39, 55.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.12G/9.98G [00:05<01:57, 75.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.16G/9.98G [00:05<01:28, 99.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.21G/9.98G [00:05<01:12, 122MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.24G/9.98G [00:05<01:02, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.27G/9.98G [00:05<00:55, 157MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.30G/9.98G [00:05<00:51, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.33G/9.98G [00:05<00:49, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.36G/9.98G [00:06<00:52, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.39G/9.98G [00:06<00:56, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.42G/9.98G [00:06<00:56, 152MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.44G/9.98G [00:06<00:55, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.46G/9.98G [00:06<00:53, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.49G/9.98G [00:06<00:47, 179MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.52G/9.98G [00:07<00:40, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.56G/9.98G [00:07<00:33, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.60G/9.98G [00:07<00:30, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.65G/9.98G [00:07<00:27, 300MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.69G/9.98G [00:07<00:27, 307MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.73G/9.98G [00:07<00:27, 305MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.77G/9.98G [00:07<00:25, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.81G/9.98G [00:07<00:24, 340MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.86G/9.98G [00:08<00:24, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.90G/9.98G [00:08<00:34, 232MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.93G/9.98G [00:08<00:37, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.98G [00:08<00:34, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.00G/9.98G [00:08<00:29, 266MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.04G/9.98G [00:08<00:27, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.98G [00:08<00:26, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.13G/9.98G [00:09<00:25, 312MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.17G/9.98G [00:10<01:23, 93.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.21G/9.98G [00:10<01:04, 121MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.25G/9.98G [00:10<00:50, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.30G/9.98G [00:10<00:41, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.34G/9.98G [00:10<00:34, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.38G/9.98G [00:10<00:30, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.42G/9.98G [00:11<00:32, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.45G/9.98G [00:11<00:39, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.49G/9.98G [00:11<00:44, 170MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.52G/9.98G [00:11<00:45, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.54G/9.98G [00:11<00:43, 170MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.57G/9.98G [00:11<00:38, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.60G/9.98G [00:12<00:33, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.64G/9.98G [00:12<00:28, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.68G/9.98G [00:12<00:26, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.72G/9.98G [00:12<00:26, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.76G/9.98G [00:12<00:23, 301MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.80G/9.98G [00:12<00:22, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.84G/9.98G [00:12<00:21, 331MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.88G/9.98G [00:12<00:22, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.93G/9.98G [00:13<00:24, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.97G/9.98G [00:13<00:22, 312MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.01G/9.98G [00:13<00:21, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.05G/9.98G [00:13<00:20, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.09G/9.98G [00:13<00:20, 344MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.14G/9.98G [00:13<00:19, 343MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.18G/9.98G [00:13<00:19, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.22G/9.98G [00:13<00:18, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.26G/9.98G [00:14<00:18, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.98G [00:14<00:19, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.34G/9.98G [00:14<00:19, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.39G/9.98G [00:14<00:19, 331MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.43G/9.98G [00:14<00:19, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.47G/9.98G [00:14<00:20, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.51G/9.98G [00:14<00:19, 327MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.55G/9.98G [00:14<00:19, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.60G/9.98G [00:15<00:19, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.64G/9.98G [00:15<00:18, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.68G/9.98G [00:15<00:17, 360MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.72G/9.98G [00:15<00:16, 368MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.76G/9.98G [00:15<00:16, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.81G/9.98G [00:15<00:16, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.85G/9.98G [00:15<00:16, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.89G/9.98G [00:15<00:16, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.93G/9.98G [00:15<00:17, 347MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.97G/9.98G [00:16<00:16, 355MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.02G/9.98G [00:16<00:17, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.06G/9.98G [00:16<00:16, 350MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.10G/9.98G [00:16<00:16, 351MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.14G/9.98G [00:16<00:16, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.18G/9.98G [00:16<00:16, 344MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.23G/9.98G [00:16<00:16, 340MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.27G/9.98G [00:16<00:17, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.31G/9.98G [00:17<00:17, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.35G/9.98G [00:17<00:17, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.39G/9.98G [00:17<00:16, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.44G/9.98G [00:17<00:16, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.48G/9.98G [00:17<00:16, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.52G/9.98G [00:17<00:16, 332MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.56G/9.98G [00:17<00:15, 339MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.60G/9.98G [00:17<00:15, 350MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.65G/9.98G [00:18<00:14, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.69G/9.98G [00:18<00:15, 352MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.73G/9.98G [00:18<00:15, 334MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.77G/9.98G [00:18<00:16, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.81G/9.98G [00:18<00:15, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.85G/9.98G [00:18<00:15, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.90G/9.98G [00:18<00:16, 315MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.94G/9.98G [00:18<00:16, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.97G/9.98G [00:19<00:16, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 5.00G/9.98G [00:19<00:15, 311MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 5.03G/9.98G [00:19<00:16, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.06G/9.98G [00:19<00:16, 305MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.11G/9.98G [00:19<00:15, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.14G/9.98G [00:19<00:15, 311MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.18G/9.98G [00:19<00:15, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.22G/9.98G [00:19<00:14, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.26G/9.98G [00:20<00:14, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.31G/9.98G [00:20<00:13, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.35G/9.98G [00:20<00:12, 359MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.39G/9.98G [00:20<00:19, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.43G/9.98G [00:20<00:16, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.47G/9.98G [00:20<00:15, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.52G/9.98G [00:20<00:13, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.56G/9.98G [00:20<00:13, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.60G/9.98G [00:21<00:12, 351MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.64G/9.98G [00:21<00:11, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.68G/9.98G [00:21<00:11, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.73G/9.98G [00:21<00:11, 371MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.77G/9.98G [00:21<00:11, 377MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.81G/9.98G [00:21<00:10, 386MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.85G/9.98G [00:21<00:10, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.89G/9.98G [00:21<00:10, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.93G/9.98G [00:21<00:10, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.98G/9.98G [00:22<00:10, 377MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.02G/9.98G [00:22<00:10, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.06G/9.98G [00:22<00:10, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.10G/9.98G [00:22<00:10, 367MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.14G/9.98G [00:22<00:10, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.19G/9.98G [00:22<00:10, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.23G/9.98G [00:22<00:10, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.27G/9.98G [00:22<00:10, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.31G/9.98G [00:22<00:10, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.35G/9.98G [00:23<00:09, 365MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.40G/9.98G [00:23<00:10, 341MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.44G/9.98G [00:23<00:10, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.48G/9.98G [00:23<00:11, 315MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.52G/9.98G [00:23<00:11, 311MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.55G/9.98G [00:23<00:11, 311MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.59G/9.98G [00:23<00:11, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.62G/9.98G [00:23<00:11, 301MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.66G/9.98G [00:24<00:10, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.70G/9.98G [00:24<00:10, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.73G/9.98G [00:24<00:10, 300MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.77G/9.98G [00:24<00:10, 309MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.82G/9.98G [00:24<00:10, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.85G/9.98G [00:24<00:10, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.88G/9.98G [00:24<00:10, 307MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.91G/9.98G [00:24<00:10, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.95G/9.98G [00:25<00:09, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.99G/9.98G [00:25<00:08, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.04G/9.98G [00:25<00:08, 351MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.08G/9.98G [00:25<00:08, 358MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.12G/9.98G [00:25<00:07, 364MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.16G/9.98G [00:25<00:07, 365MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.20G/9.98G [00:25<00:07, 365MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.25G/9.98G [00:25<00:07, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.29G/9.98G [00:25<00:07, 365MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.33G/9.98G [00:26<00:07, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.37G/9.98G [00:26<00:07, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.41G/9.98G [00:26<00:06, 368MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.46G/9.98G [00:26<00:06, 368MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.50G/9.98G [00:26<00:06, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.54G/9.98G [00:26<00:06, 368MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.58G/9.98G [00:26<00:06, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.62G/9.98G [00:26<00:06, 368MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.67G/9.98G [00:26<00:06, 367MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.71G/9.98G [00:27<00:06, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.75G/9.98G [00:27<00:06, 367MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.79G/9.98G [00:27<00:06, 360MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.83G/9.98G [00:27<00:05, 364MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.87G/9.98G [00:27<00:05, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.92G/9.98G [00:27<00:05, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.96G/9.98G [00:27<00:05, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.00G/9.98G [00:27<00:05, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.04G/9.98G [00:28<00:05, 368MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.08G/9.98G [00:28<00:05, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.13G/9.98G [00:28<00:05, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.17G/9.98G [00:28<00:05, 355MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.21G/9.98G [00:28<00:04, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.25G/9.98G [00:28<00:04, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.29G/9.98G [00:28<00:04, 353MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.34G/9.98G [00:28<00:04, 350MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.38G/9.98G [00:28<00:04, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.42G/9.98G [00:29<00:04, 350MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.46G/9.98G [00:29<00:04, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.50G/9.98G [00:29<00:04, 364MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.55G/9.98G [00:29<00:03, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.59G/9.98G [00:29<00:03, 365MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.63G/9.98G [00:29<00:03, 370MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.67G/9.98G [00:29<00:03, 374MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.71G/9.98G [00:29<00:03, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.76G/9.98G [00:31<00:15, 80.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.80G/9.98G [00:31<00:11, 105MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.84G/9.98G [00:31<00:08, 134MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.88G/9.98G [00:31<00:06, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.92G/9.98G [00:31<00:05, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.97G/9.98G [00:31<00:04, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 9.01G/9.98G [00:32<00:03, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.05G/9.98G [00:32<00:03, 293MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.09G/9.98G [00:32<00:02, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.13G/9.98G [00:32<00:02, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.18G/9.98G [00:32<00:02, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.22G/9.98G [00:32<00:02, 345MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.26G/9.98G [00:32<00:02, 332MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.30G/9.98G [00:32<00:02, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.34G/9.98G [00:32<00:01, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.38G/9.98G [00:33<00:01, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.43G/9.98G [00:33<00:01, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.47G/9.98G [00:33<00:01, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.51G/9.98G [00:33<00:01, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.55G/9.98G [00:33<00:01, 332MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.59G/9.98G [00:33<00:01, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.64G/9.98G [00:33<00:01, 332MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.68G/9.98G [00:34<00:00, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.72G/9.98G [00:34<00:00, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.76G/9.98G [00:34<00:00, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.80G/9.98G [00:34<00:00, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.84G/9.98G [00:34<00:00, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.87G/9.98G [00:34<00:00, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.90G/9.98G [00:34<00:00, 308MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.93G/9.98G [00:34<00:00, 305MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:35<00:00, 285MB/s]\n",
            "Downloading shards:  50% 1/2 [00:35<00:35, 35.22s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 41.9M/3.50G [00:00<00:09, 380MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 83.9M/3.50G [00:00<00:08, 380MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 126M/3.50G [00:00<00:08, 378MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 168M/3.50G [00:00<00:08, 374MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 210M/3.50G [00:00<00:08, 367MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 252M/3.50G [00:00<00:08, 366MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 294M/3.50G [00:00<00:08, 366MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 336M/3.50G [00:00<00:08, 372MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 377M/3.50G [00:01<00:08, 377MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 419M/3.50G [00:01<00:08, 381MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 461M/3.50G [00:01<00:07, 382MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 503M/3.50G [00:01<00:07, 385MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 545M/3.50G [00:01<00:07, 388MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 587M/3.50G [00:01<00:07, 389MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 629M/3.50G [00:01<00:07, 386MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 671M/3.50G [00:01<00:07, 387MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 713M/3.50G [00:01<00:07, 389MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 755M/3.50G [00:01<00:07, 388MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 797M/3.50G [00:02<00:07, 379MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 839M/3.50G [00:02<00:07, 374MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 881M/3.50G [00:02<00:06, 379MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 923M/3.50G [00:02<00:07, 366MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 965M/3.50G [00:02<00:06, 370MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.01G/3.50G [00:02<00:06, 372MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.05G/3.50G [00:02<00:06, 376MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.09G/3.50G [00:02<00:06, 374MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.13G/3.50G [00:03<00:06, 370MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.17G/3.50G [00:03<00:06, 366MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.22G/3.50G [00:03<00:06, 369MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.26G/3.50G [00:03<00:06, 367MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.30G/3.50G [00:03<00:06, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.34G/3.50G [00:03<00:06, 352MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.38G/3.50G [00:03<00:06, 351MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.43G/3.50G [00:03<00:05, 352MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.47G/3.50G [00:03<00:05, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.51G/3.50G [00:04<00:05, 359MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.55G/3.50G [00:04<00:05, 356MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.59G/3.50G [00:04<00:05, 357MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.64G/3.50G [00:04<00:05, 362MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.68G/3.50G [00:04<00:05, 357MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.72G/3.50G [00:04<00:04, 359MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.76G/3.50G [00:04<00:04, 363MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.80G/3.50G [00:04<00:04, 363MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.85G/3.50G [00:04<00:04, 358MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.89G/3.50G [00:05<00:04, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.93G/3.50G [00:05<00:04, 357MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.97G/3.50G [00:05<00:04, 364MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.01G/3.50G [00:05<00:04, 368MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.06G/3.50G [00:05<00:03, 374MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.10G/3.50G [00:05<00:03, 373MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.14G/3.50G [00:05<00:03, 370MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.18G/3.50G [00:05<00:03, 369MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.22G/3.50G [00:06<00:03, 371MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.26G/3.50G [00:06<00:03, 367MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 2.31G/3.50G [00:06<00:03, 367MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 2.35G/3.50G [00:06<00:03, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 2.39G/3.50G [00:06<00:03, 362MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 2.43G/3.50G [00:06<00:02, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 2.47G/3.50G [00:06<00:02, 363MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 2.52G/3.50G [00:06<00:02, 367MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 2.56G/3.50G [00:06<00:02, 370MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 2.60G/3.50G [00:07<00:02, 366MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.64G/3.50G [00:07<00:02, 367MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.68G/3.50G [00:07<00:02, 374MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.73G/3.50G [00:07<00:02, 377MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.77G/3.50G [00:07<00:01, 378MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.81G/3.50G [00:07<00:01, 377MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.85G/3.50G [00:07<00:01, 383MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.89G/3.50G [00:07<00:01, 385MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.94G/3.50G [00:07<00:01, 384MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.98G/3.50G [00:08<00:01, 382MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.02G/3.50G [00:08<00:01, 378MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.06G/3.50G [00:08<00:01, 373MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 3.10G/3.50G [00:08<00:01, 373MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 3.15G/3.50G [00:08<00:00, 373MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 3.19G/3.50G [00:08<00:00, 334MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 3.23G/3.50G [00:08<00:00, 307MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 3.27G/3.50G [00:08<00:00, 280MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 3.30G/3.50G [00:09<00:00, 265MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 3.33G/3.50G [00:09<00:00, 253MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 3.37G/3.50G [00:09<00:00, 246MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 3.40G/3.50G [00:09<00:00, 249MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 3.43G/3.50G [00:09<00:00, 262MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:09<00:00, 355MB/s]\n",
            "Downloading shards: 100% 2/2 [00:45<00:00, 22.62s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:26<00:00, 13.26s/it]\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 1.02MB/s]\n",
            "Running llama_sequential\n",
            "Starting ...\n",
            "llama_sequential Done: 329.4955596923828\n",
            "Running llama_pack\n",
            "Packing ...\n",
            "model.layers.0.self_attn.k_proj\n",
            "/content/SqueezeLLM/squeezellm/quant.py:126: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  outliers = outliers.to_sparse(layout=torch.sparse_csr)\n",
            "self.numvals:  970119\n",
            "self.rows:  4097\n",
            "self.num_threads:  97024\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.self_attn.o_proj\n",
            "self.numvals:  645210\n",
            "self.rows:  4097\n",
            "self.num_threads:  64640\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.self_attn.q_proj\n",
            "self.numvals:  1189179\n",
            "self.rows:  4097\n",
            "self.num_threads:  119040\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.self_attn.v_proj\n",
            "self.numvals:  454257\n",
            "self.rows:  4097\n",
            "self.num_threads:  45440\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.mlp.down_proj\n",
            "self.numvals:  152392\n",
            "self.rows:  4097\n",
            "self.num_threads:  15360\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.mlp.gate_proj\n",
            "self.numvals:  171929\n",
            "self.rows:  11009\n",
            "self.num_threads:  17280\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.mlp.up_proj\n",
            "self.numvals:  141932\n",
            "self.rows:  11009\n",
            "self.num_threads:  14208\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.self_attn.k_proj\n",
            "self.numvals:  922543\n",
            "self.rows:  4097\n",
            "self.num_threads:  92288\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.self_attn.o_proj\n",
            "self.numvals:  795880\n",
            "self.rows:  4097\n",
            "self.num_threads:  79616\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.self_attn.q_proj\n",
            "self.numvals:  1094630\n",
            "self.rows:  4097\n",
            "self.num_threads:  109568\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.self_attn.v_proj\n",
            "self.numvals:  433267\n",
            "self.rows:  4097\n",
            "self.num_threads:  43392\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.mlp.down_proj\n",
            "self.numvals:  125890\n",
            "self.rows:  4097\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.mlp.gate_proj\n",
            "self.numvals:  148839\n",
            "self.rows:  11009\n",
            "self.num_threads:  14976\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.mlp.up_proj\n",
            "self.numvals:  124182\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.self_attn.k_proj\n",
            "self.numvals:  663813\n",
            "self.rows:  4097\n",
            "self.num_threads:  66432\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.self_attn.o_proj\n",
            "self.numvals:  63959\n",
            "self.rows:  4097\n",
            "self.num_threads:  6400\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.self_attn.q_proj\n",
            "self.numvals:  488217\n",
            "self.rows:  4097\n",
            "self.num_threads:  48896\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.self_attn.v_proj\n",
            "self.numvals:  62655\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.mlp.down_proj\n",
            "self.numvals:  116601\n",
            "self.rows:  4097\n",
            "self.num_threads:  11776\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.mlp.gate_proj\n",
            "self.numvals:  125550\n",
            "self.rows:  11009\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.mlp.up_proj\n",
            "self.numvals:  119211\n",
            "self.rows:  11009\n",
            "self.num_threads:  12032\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.self_attn.k_proj\n",
            "self.numvals:  387213\n",
            "self.rows:  4097\n",
            "self.num_threads:  38784\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.self_attn.o_proj\n",
            "self.numvals:  69683\n",
            "self.rows:  4097\n",
            "self.num_threads:  7040\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.self_attn.q_proj\n",
            "self.numvals:  308852\n",
            "self.rows:  4097\n",
            "self.num_threads:  30976\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.self_attn.v_proj\n",
            "self.numvals:  70757\n",
            "self.rows:  4097\n",
            "self.num_threads:  7168\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.mlp.down_proj\n",
            "self.numvals:  116988\n",
            "self.rows:  4097\n",
            "self.num_threads:  11776\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.mlp.gate_proj\n",
            "self.numvals:  131833\n",
            "self.rows:  11009\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.mlp.up_proj\n",
            "self.numvals:  124096\n",
            "self.rows:  11009\n",
            "self.num_threads:  12416\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.self_attn.k_proj\n",
            "self.numvals:  308518\n",
            "self.rows:  4097\n",
            "self.num_threads:  30976\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.self_attn.o_proj\n",
            "self.numvals:  55642\n",
            "self.rows:  4097\n",
            "self.num_threads:  5632\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.self_attn.q_proj\n",
            "self.numvals:  238812\n",
            "self.rows:  4097\n",
            "self.num_threads:  23936\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.self_attn.v_proj\n",
            "self.numvals:  57128\n",
            "self.rows:  4097\n",
            "self.num_threads:  5760\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.mlp.down_proj\n",
            "self.numvals:  125460\n",
            "self.rows:  4097\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.mlp.gate_proj\n",
            "self.numvals:  158667\n",
            "self.rows:  11009\n",
            "self.num_threads:  15872\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.mlp.up_proj\n",
            "self.numvals:  122864\n",
            "self.rows:  11009\n",
            "self.num_threads:  12288\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.self_attn.k_proj\n",
            "self.numvals:  263471\n",
            "self.rows:  4097\n",
            "self.num_threads:  26368\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.self_attn.o_proj\n",
            "self.numvals:  57778\n",
            "self.rows:  4097\n",
            "self.num_threads:  5888\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.self_attn.q_proj\n",
            "self.numvals:  199854\n",
            "self.rows:  4097\n",
            "self.num_threads:  20096\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.self_attn.v_proj\n",
            "self.numvals:  65777\n",
            "self.rows:  4097\n",
            "self.num_threads:  6656\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.mlp.down_proj\n",
            "self.numvals:  127029\n",
            "self.rows:  4097\n",
            "self.num_threads:  12800\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.mlp.gate_proj\n",
            "self.numvals:  180681\n",
            "self.rows:  11009\n",
            "self.num_threads:  18176\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.mlp.up_proj\n",
            "self.numvals:  124472\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.self_attn.k_proj\n",
            "self.numvals:  325098\n",
            "self.rows:  4097\n",
            "self.num_threads:  32512\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.self_attn.o_proj\n",
            "self.numvals:  69741\n",
            "self.rows:  4097\n",
            "self.num_threads:  7040\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.self_attn.q_proj\n",
            "self.numvals:  280881\n",
            "self.rows:  4097\n",
            "self.num_threads:  28160\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.self_attn.v_proj\n",
            "self.numvals:  81619\n",
            "self.rows:  4097\n",
            "self.num_threads:  8192\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.mlp.down_proj\n",
            "self.numvals:  125220\n",
            "self.rows:  4097\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.mlp.gate_proj\n",
            "self.numvals:  195634\n",
            "self.rows:  11009\n",
            "self.num_threads:  19584\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.mlp.up_proj\n",
            "self.numvals:  125206\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.self_attn.k_proj\n",
            "self.numvals:  305032\n",
            "self.rows:  4097\n",
            "self.num_threads:  30592\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.self_attn.o_proj\n",
            "self.numvals:  77850\n",
            "self.rows:  4097\n",
            "self.num_threads:  7808\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.self_attn.q_proj\n",
            "self.numvals:  281158\n",
            "self.rows:  4097\n",
            "self.num_threads:  28160\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.self_attn.v_proj\n",
            "self.numvals:  86367\n",
            "self.rows:  4097\n",
            "self.num_threads:  8704\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.mlp.down_proj\n",
            "self.numvals:  124374\n",
            "self.rows:  4097\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.mlp.gate_proj\n",
            "self.numvals:  206517\n",
            "self.rows:  11009\n",
            "self.num_threads:  20736\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.mlp.up_proj\n",
            "self.numvals:  125178\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.self_attn.k_proj\n",
            "self.numvals:  278923\n",
            "self.rows:  4097\n",
            "self.num_threads:  27904\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.self_attn.o_proj\n",
            "self.numvals:  60150\n",
            "self.rows:  4097\n",
            "self.num_threads:  6016\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.self_attn.q_proj\n",
            "self.numvals:  255153\n",
            "self.rows:  4097\n",
            "self.num_threads:  25600\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.self_attn.v_proj\n",
            "self.numvals:  71661\n",
            "self.rows:  4097\n",
            "self.num_threads:  7168\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.mlp.down_proj\n",
            "self.numvals:  131950\n",
            "self.rows:  4097\n",
            "self.num_threads:  13312\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.mlp.gate_proj\n",
            "self.numvals:  210313\n",
            "self.rows:  11009\n",
            "self.num_threads:  21120\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.mlp.up_proj\n",
            "self.numvals:  125204\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.self_attn.k_proj\n",
            "self.numvals:  235224\n",
            "self.rows:  4097\n",
            "self.num_threads:  23552\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.self_attn.o_proj\n",
            "self.numvals:  60972\n",
            "self.rows:  4097\n",
            "self.num_threads:  6144\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.self_attn.q_proj\n",
            "self.numvals:  192528\n",
            "self.rows:  4097\n",
            "self.num_threads:  19328\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.self_attn.v_proj\n",
            "self.numvals:  72562\n",
            "self.rows:  4097\n",
            "self.num_threads:  7296\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.mlp.down_proj\n",
            "self.numvals:  126979\n",
            "self.rows:  4097\n",
            "self.num_threads:  12800\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.mlp.gate_proj\n",
            "self.numvals:  224394\n",
            "self.rows:  11009\n",
            "self.num_threads:  22528\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.mlp.up_proj\n",
            "self.numvals:  129618\n",
            "self.rows:  11009\n",
            "self.num_threads:  13056\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.self_attn.k_proj\n",
            "self.numvals:  278686\n",
            "self.rows:  4097\n",
            "self.num_threads:  27904\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.self_attn.o_proj\n",
            "self.numvals:  56413\n",
            "self.rows:  4097\n",
            "self.num_threads:  5760\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.self_attn.q_proj\n",
            "self.numvals:  217730\n",
            "self.rows:  4097\n",
            "self.num_threads:  21888\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.self_attn.v_proj\n",
            "self.numvals:  65064\n",
            "self.rows:  4097\n",
            "self.num_threads:  6528\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.mlp.down_proj\n",
            "self.numvals:  133059\n",
            "self.rows:  4097\n",
            "self.num_threads:  13312\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.mlp.gate_proj\n",
            "self.numvals:  258714\n",
            "self.rows:  11009\n",
            "self.num_threads:  25984\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.mlp.up_proj\n",
            "self.numvals:  134139\n",
            "self.rows:  11009\n",
            "self.num_threads:  13440\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.self_attn.k_proj\n",
            "self.numvals:  327886\n",
            "self.rows:  4097\n",
            "self.num_threads:  32896\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.self_attn.o_proj\n",
            "self.numvals:  70259\n",
            "self.rows:  4097\n",
            "self.num_threads:  7040\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.self_attn.q_proj\n",
            "self.numvals:  312503\n",
            "self.rows:  4097\n",
            "self.num_threads:  31360\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.self_attn.v_proj\n",
            "self.numvals:  73444\n",
            "self.rows:  4097\n",
            "self.num_threads:  7424\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.mlp.down_proj\n",
            "self.numvals:  129076\n",
            "self.rows:  4097\n",
            "self.num_threads:  12928\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.mlp.gate_proj\n",
            "self.numvals:  236844\n",
            "self.rows:  11009\n",
            "self.num_threads:  23808\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.mlp.up_proj\n",
            "self.numvals:  132729\n",
            "self.rows:  11009\n",
            "self.num_threads:  13312\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.self_attn.k_proj\n",
            "self.numvals:  258005\n",
            "self.rows:  4097\n",
            "self.num_threads:  25856\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.self_attn.o_proj\n",
            "self.numvals:  65754\n",
            "self.rows:  4097\n",
            "self.num_threads:  6656\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.self_attn.q_proj\n",
            "self.numvals:  211605\n",
            "self.rows:  4097\n",
            "self.num_threads:  21248\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.self_attn.v_proj\n",
            "self.numvals:  75588\n",
            "self.rows:  4097\n",
            "self.num_threads:  7680\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.mlp.down_proj\n",
            "self.numvals:  133450\n",
            "self.rows:  4097\n",
            "self.num_threads:  13440\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.mlp.gate_proj\n",
            "self.numvals:  225138\n",
            "self.rows:  11009\n",
            "self.num_threads:  22528\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.mlp.up_proj\n",
            "self.numvals:  128162\n",
            "self.rows:  11009\n",
            "self.num_threads:  12928\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.self_attn.k_proj\n",
            "self.numvals:  238148\n",
            "self.rows:  4097\n",
            "self.num_threads:  23936\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.self_attn.o_proj\n",
            "self.numvals:  62570\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.self_attn.q_proj\n",
            "self.numvals:  204668\n",
            "self.rows:  4097\n",
            "self.num_threads:  20480\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.self_attn.v_proj\n",
            "self.numvals:  71520\n",
            "self.rows:  4097\n",
            "self.num_threads:  7168\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.mlp.down_proj\n",
            "self.numvals:  133344\n",
            "self.rows:  4097\n",
            "self.num_threads:  13440\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.mlp.gate_proj\n",
            "self.numvals:  226285\n",
            "self.rows:  11009\n",
            "self.num_threads:  22656\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.mlp.up_proj\n",
            "self.numvals:  131032\n",
            "self.rows:  11009\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.self_attn.k_proj\n",
            "self.numvals:  241345\n",
            "self.rows:  4097\n",
            "self.num_threads:  24192\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.self_attn.o_proj\n",
            "self.numvals:  56949\n",
            "self.rows:  4097\n",
            "self.num_threads:  5760\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.self_attn.q_proj\n",
            "self.numvals:  214512\n",
            "self.rows:  4097\n",
            "self.num_threads:  21504\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.self_attn.v_proj\n",
            "self.numvals:  64784\n",
            "self.rows:  4097\n",
            "self.num_threads:  6528\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.mlp.down_proj\n",
            "self.numvals:  133018\n",
            "self.rows:  4097\n",
            "self.num_threads:  13312\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.mlp.gate_proj\n",
            "self.numvals:  214459\n",
            "self.rows:  11009\n",
            "self.num_threads:  21504\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.mlp.up_proj\n",
            "self.numvals:  131301\n",
            "self.rows:  11009\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.self_attn.k_proj\n",
            "self.numvals:  214606\n",
            "self.rows:  4097\n",
            "self.num_threads:  21504\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.self_attn.o_proj\n",
            "self.numvals:  56942\n",
            "self.rows:  4097\n",
            "self.num_threads:  5760\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.self_attn.q_proj\n",
            "self.numvals:  176288\n",
            "self.rows:  4097\n",
            "self.num_threads:  17664\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.self_attn.v_proj\n",
            "self.numvals:  60205\n",
            "self.rows:  4097\n",
            "self.num_threads:  6144\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.mlp.down_proj\n",
            "self.numvals:  129580\n",
            "self.rows:  4097\n",
            "self.num_threads:  13056\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.mlp.gate_proj\n",
            "self.numvals:  227875\n",
            "self.rows:  11009\n",
            "self.num_threads:  22912\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.mlp.up_proj\n",
            "self.numvals:  134303\n",
            "self.rows:  11009\n",
            "self.num_threads:  13440\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.self_attn.k_proj\n",
            "self.numvals:  242480\n",
            "self.rows:  4097\n",
            "self.num_threads:  24320\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.self_attn.o_proj\n",
            "self.numvals:  55256\n",
            "self.rows:  4097\n",
            "self.num_threads:  5632\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.self_attn.q_proj\n",
            "self.numvals:  203860\n",
            "self.rows:  4097\n",
            "self.num_threads:  20480\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.self_attn.v_proj\n",
            "self.numvals:  61546\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.mlp.down_proj\n",
            "self.numvals:  125586\n",
            "self.rows:  4097\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.mlp.gate_proj\n",
            "self.numvals:  213571\n",
            "self.rows:  11009\n",
            "self.num_threads:  21376\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.mlp.up_proj\n",
            "self.numvals:  130379\n",
            "self.rows:  11009\n",
            "self.num_threads:  13056\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.self_attn.k_proj\n",
            "self.numvals:  196730\n",
            "self.rows:  4097\n",
            "self.num_threads:  19712\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.self_attn.o_proj\n",
            "self.numvals:  54714\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.self_attn.q_proj\n",
            "self.numvals:  161350\n",
            "self.rows:  4097\n",
            "self.num_threads:  16256\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.self_attn.v_proj\n",
            "self.numvals:  56201\n",
            "self.rows:  4097\n",
            "self.num_threads:  5632\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.mlp.down_proj\n",
            "self.numvals:  121513\n",
            "self.rows:  4097\n",
            "self.num_threads:  12160\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.mlp.gate_proj\n",
            "self.numvals:  209486\n",
            "self.rows:  11009\n",
            "self.num_threads:  20992\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.mlp.up_proj\n",
            "self.numvals:  123033\n",
            "self.rows:  11009\n",
            "self.num_threads:  12416\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.self_attn.k_proj\n",
            "self.numvals:  169804\n",
            "self.rows:  4097\n",
            "self.num_threads:  17024\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.self_attn.o_proj\n",
            "self.numvals:  53395\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.self_attn.q_proj\n",
            "self.numvals:  142135\n",
            "self.rows:  4097\n",
            "self.num_threads:  14336\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.self_attn.v_proj\n",
            "self.numvals:  58886\n",
            "self.rows:  4097\n",
            "self.num_threads:  6016\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.mlp.down_proj\n",
            "self.numvals:  118938\n",
            "self.rows:  4097\n",
            "self.num_threads:  11904\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.mlp.gate_proj\n",
            "self.numvals:  199523\n",
            "self.rows:  11009\n",
            "self.num_threads:  19968\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.mlp.up_proj\n",
            "self.numvals:  122727\n",
            "self.rows:  11009\n",
            "self.num_threads:  12288\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.self_attn.k_proj\n",
            "self.numvals:  193156\n",
            "self.rows:  4097\n",
            "self.num_threads:  19328\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.self_attn.o_proj\n",
            "self.numvals:  55666\n",
            "self.rows:  4097\n",
            "self.num_threads:  5632\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.self_attn.q_proj\n",
            "self.numvals:  160467\n",
            "self.rows:  4097\n",
            "self.num_threads:  16128\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.self_attn.v_proj\n",
            "self.numvals:  62282\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.mlp.down_proj\n",
            "self.numvals:  120820\n",
            "self.rows:  4097\n",
            "self.num_threads:  12160\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.mlp.gate_proj\n",
            "self.numvals:  184897\n",
            "self.rows:  11009\n",
            "self.num_threads:  18560\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.mlp.up_proj\n",
            "self.numvals:  121242\n",
            "self.rows:  11009\n",
            "self.num_threads:  12160\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.self_attn.k_proj\n",
            "self.numvals:  211208\n",
            "self.rows:  4097\n",
            "self.num_threads:  21248\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.self_attn.o_proj\n",
            "self.numvals:  49512\n",
            "self.rows:  4097\n",
            "self.num_threads:  4992\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.self_attn.q_proj\n",
            "self.numvals:  178956\n",
            "self.rows:  4097\n",
            "self.num_threads:  17920\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.self_attn.v_proj\n",
            "self.numvals:  50096\n",
            "self.rows:  4097\n",
            "self.num_threads:  5120\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.mlp.down_proj\n",
            "self.numvals:  118973\n",
            "self.rows:  4097\n",
            "self.num_threads:  11904\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.mlp.gate_proj\n",
            "self.numvals:  169894\n",
            "self.rows:  11009\n",
            "self.num_threads:  17024\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.mlp.up_proj\n",
            "self.numvals:  119086\n",
            "self.rows:  11009\n",
            "self.num_threads:  12032\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.self_attn.k_proj\n",
            "self.numvals:  214202\n",
            "self.rows:  4097\n",
            "self.num_threads:  21504\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.self_attn.o_proj\n",
            "self.numvals:  47686\n",
            "self.rows:  4097\n",
            "self.num_threads:  4864\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.self_attn.q_proj\n",
            "self.numvals:  177748\n",
            "self.rows:  4097\n",
            "self.num_threads:  17792\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.self_attn.v_proj\n",
            "self.numvals:  52825\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.mlp.down_proj\n",
            "self.numvals:  113816\n",
            "self.rows:  4097\n",
            "self.num_threads:  11392\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.mlp.gate_proj\n",
            "self.numvals:  163179\n",
            "self.rows:  11009\n",
            "self.num_threads:  16384\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.mlp.up_proj\n",
            "self.numvals:  117050\n",
            "self.rows:  11009\n",
            "self.num_threads:  11776\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.self_attn.k_proj\n",
            "self.numvals:  177105\n",
            "self.rows:  4097\n",
            "self.num_threads:  17792\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.self_attn.o_proj\n",
            "self.numvals:  54364\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.self_attn.q_proj\n",
            "self.numvals:  156110\n",
            "self.rows:  4097\n",
            "self.num_threads:  15616\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.self_attn.v_proj\n",
            "self.numvals:  53736\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.mlp.down_proj\n",
            "self.numvals:  113762\n",
            "self.rows:  4097\n",
            "self.num_threads:  11392\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.mlp.gate_proj\n",
            "self.numvals:  156434\n",
            "self.rows:  11009\n",
            "self.num_threads:  15744\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.mlp.up_proj\n",
            "self.numvals:  114960\n",
            "self.rows:  11009\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.self_attn.k_proj\n",
            "self.numvals:  144807\n",
            "self.rows:  4097\n",
            "self.num_threads:  14592\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.self_attn.o_proj\n",
            "self.numvals:  53112\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.self_attn.q_proj\n",
            "self.numvals:  131461\n",
            "self.rows:  4097\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.self_attn.v_proj\n",
            "self.numvals:  54433\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.mlp.down_proj\n",
            "self.numvals:  114917\n",
            "self.rows:  4097\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.mlp.gate_proj\n",
            "self.numvals:  147160\n",
            "self.rows:  11009\n",
            "self.num_threads:  14720\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.mlp.up_proj\n",
            "self.numvals:  114906\n",
            "self.rows:  11009\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.self_attn.k_proj\n",
            "self.numvals:  235696\n",
            "self.rows:  4097\n",
            "self.num_threads:  23680\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.self_attn.o_proj\n",
            "self.numvals:  49641\n",
            "self.rows:  4097\n",
            "self.num_threads:  4992\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.self_attn.q_proj\n",
            "self.numvals:  205627\n",
            "self.rows:  4097\n",
            "self.num_threads:  20608\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.self_attn.v_proj\n",
            "self.numvals:  53212\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.mlp.down_proj\n",
            "self.numvals:  113112\n",
            "self.rows:  4097\n",
            "self.num_threads:  11392\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.mlp.gate_proj\n",
            "self.numvals:  148071\n",
            "self.rows:  11009\n",
            "self.num_threads:  14848\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.mlp.up_proj\n",
            "self.numvals:  114077\n",
            "self.rows:  11009\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.self_attn.k_proj\n",
            "self.numvals:  158391\n",
            "self.rows:  4097\n",
            "self.num_threads:  15872\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.self_attn.o_proj\n",
            "self.numvals:  48859\n",
            "self.rows:  4097\n",
            "self.num_threads:  4992\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.self_attn.q_proj\n",
            "self.numvals:  151578\n",
            "self.rows:  4097\n",
            "self.num_threads:  15232\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.self_attn.v_proj\n",
            "self.numvals:  52518\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.mlp.down_proj\n",
            "self.numvals:  114832\n",
            "self.rows:  4097\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.mlp.gate_proj\n",
            "self.numvals:  154988\n",
            "self.rows:  11009\n",
            "self.num_threads:  15616\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.mlp.up_proj\n",
            "self.numvals:  113654\n",
            "self.rows:  11009\n",
            "self.num_threads:  11392\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.self_attn.k_proj\n",
            "self.numvals:  182032\n",
            "self.rows:  4097\n",
            "self.num_threads:  18304\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.self_attn.o_proj\n",
            "self.numvals:  74384\n",
            "self.rows:  4097\n",
            "self.num_threads:  7552\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.self_attn.q_proj\n",
            "self.numvals:  167716\n",
            "self.rows:  4097\n",
            "self.num_threads:  16896\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.self_attn.v_proj\n",
            "self.numvals:  62451\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.mlp.down_proj\n",
            "self.numvals:  118607\n",
            "self.rows:  4097\n",
            "self.num_threads:  11904\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.mlp.gate_proj\n",
            "self.numvals:  180747\n",
            "self.rows:  11009\n",
            "self.num_threads:  18176\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.mlp.up_proj\n",
            "self.numvals:  118230\n",
            "self.rows:  11009\n",
            "self.num_threads:  11904\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.self_attn.k_proj\n",
            "self.numvals:  109389\n",
            "self.rows:  4097\n",
            "self.num_threads:  11008\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.self_attn.o_proj\n",
            "self.numvals:  53485\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.self_attn.q_proj\n",
            "self.numvals:  104370\n",
            "self.rows:  4097\n",
            "self.num_threads:  10496\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.self_attn.v_proj\n",
            "self.numvals:  52381\n",
            "self.rows:  4097\n",
            "self.num_threads:  5248\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.mlp.down_proj\n",
            "self.numvals:  121148\n",
            "self.rows:  4097\n",
            "self.num_threads:  12160\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.mlp.gate_proj\n",
            "self.numvals:  198006\n",
            "self.rows:  11009\n",
            "self.num_threads:  19840\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.mlp.up_proj\n",
            "self.numvals:  115840\n",
            "self.rows:  11009\n",
            "self.num_threads:  11648\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.self_attn.k_proj\n",
            "self.numvals:  135169\n",
            "self.rows:  4097\n",
            "self.num_threads:  13568\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.self_attn.o_proj\n",
            "self.numvals:  58035\n",
            "self.rows:  4097\n",
            "self.num_threads:  5888\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.self_attn.q_proj\n",
            "self.numvals:  127043\n",
            "self.rows:  4097\n",
            "self.num_threads:  12800\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.self_attn.v_proj\n",
            "self.numvals:  61233\n",
            "self.rows:  4097\n",
            "self.num_threads:  6144\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.mlp.down_proj\n",
            "self.numvals:  122111\n",
            "self.rows:  4097\n",
            "self.num_threads:  12288\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.mlp.gate_proj\n",
            "self.numvals:  216555\n",
            "self.rows:  11009\n",
            "self.num_threads:  21760\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.mlp.up_proj\n",
            "self.numvals:  119455\n",
            "self.rows:  11009\n",
            "self.num_threads:  12032\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.self_attn.k_proj\n",
            "self.numvals:  173976\n",
            "self.rows:  4097\n",
            "self.num_threads:  17408\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.self_attn.o_proj\n",
            "self.numvals:  53784\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.self_attn.q_proj\n",
            "self.numvals:  157795\n",
            "self.rows:  4097\n",
            "self.num_threads:  15872\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.self_attn.v_proj\n",
            "self.numvals:  54955\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.mlp.down_proj\n",
            "self.numvals:  130820\n",
            "self.rows:  4097\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.mlp.gate_proj\n",
            "self.numvals:  244054\n",
            "self.rows:  11009\n",
            "self.num_threads:  24448\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.mlp.up_proj\n",
            "self.numvals:  122806\n",
            "self.rows:  11009\n",
            "self.num_threads:  12288\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.self_attn.k_proj\n",
            "self.numvals:  123407\n",
            "self.rows:  4097\n",
            "self.num_threads:  12416\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.self_attn.o_proj\n",
            "self.numvals:  52056\n",
            "self.rows:  4097\n",
            "self.num_threads:  5248\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.self_attn.q_proj\n",
            "self.numvals:  114708\n",
            "self.rows:  4097\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.self_attn.v_proj\n",
            "self.numvals:  51980\n",
            "self.rows:  4097\n",
            "self.num_threads:  5248\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.mlp.down_proj\n",
            "self.numvals:  156875\n",
            "self.rows:  4097\n",
            "self.num_threads:  15744\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.mlp.gate_proj\n",
            "self.numvals:  270572\n",
            "self.rows:  11009\n",
            "self.num_threads:  27136\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.mlp.up_proj\n",
            "self.numvals:  134820\n",
            "self.rows:  11009\n",
            "self.num_threads:  13568\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.self_attn.k_proj\n",
            "self.numvals:  159125\n",
            "self.rows:  4097\n",
            "self.num_threads:  16000\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.self_attn.o_proj\n",
            "self.numvals:  71317\n",
            "self.rows:  4097\n",
            "self.num_threads:  7168\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.self_attn.q_proj\n",
            "self.numvals:  125949\n",
            "self.rows:  4097\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.self_attn.v_proj\n",
            "self.numvals:  66123\n",
            "self.rows:  4097\n",
            "self.num_threads:  6656\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.mlp.down_proj\n",
            "self.numvals:  195508\n",
            "self.rows:  4097\n",
            "self.num_threads:  19584\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.mlp.gate_proj\n",
            "self.numvals:  266385\n",
            "self.rows:  11009\n",
            "self.num_threads:  26752\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.mlp.up_proj\n",
            "self.numvals:  168298\n",
            "self.rows:  11009\n",
            "self.num_threads:  16896\n",
            "nnz_per_thread:  10\n",
            "Done.\n",
            "llama_pack Done: 1152.7623403072357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For 3-bit quantization"
      ],
      "metadata": {
        "id": "AJO4QpdJHnol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python SqueezeLLM/quantization/nuq.py --bit 3 --model_type llama --model drive/MyDrive/llama-chunks --gradient drive/MyDrive/llama-gradients-chunks --output drive/MyDrive/llama-LUT-3bit/ --outlier_config drive/MyDrive/llama-chunks-outlierconfig/outlier_config_o0.53.json --sensitivity 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFTNkRloHqHJ",
        "outputId": "a62468c6-c059-4600-e2ba-113e6a6afa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running outlier mode with outlier folder threshold 0.53 and sensitivity 0.05\n",
            "Quantizing layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
            "Quantizing layer 0\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04984307165590593\n",
            "removing outliers by threshold\n",
            "p outlier: 1.7908066665570352\n",
            "100% 7/7 [05:10<00:00, 44.30s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l0.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l0.pkl\n",
            "Quantizing layer 1\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049876672616276714\n",
            "removing outliers by threshold\n",
            "p outlier: 1.7513487623145543\n",
            "100% 7/7 [05:41<00:00, 48.72s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l1.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l1.pkl\n",
            "Quantizing layer 2\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049625159545265946\n",
            "removing outliers by threshold\n",
            "p outlier: 0.7607544024373583\n",
            " 57% 4/7 [01:55<01:26, 28.88s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            " 71% 5/7 [03:16<01:35, 47.57s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:38<00:00, 48.41s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l2.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l2.pkl\n",
            "Quantizing layer 3\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049734362666471015\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5478799651941487\n",
            "100% 7/7 [05:42<00:00, 48.88s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l3.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l3.pkl\n",
            "Quantizing layer 4\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049656289846785946\n",
            "removing outliers by threshold\n",
            "p outlier: 0.47762727490360873\n",
            " 57% 4/7 [01:53<01:26, 28.76s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:43<00:00, 49.02s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l4.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l4.pkl\n",
            "Quantizing layer 5\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04972596242637831\n",
            "removing outliers by threshold\n",
            "p outlier: 0.45382494753506514\n",
            "100% 7/7 [05:34<00:00, 47.72s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l5.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l5.pkl\n",
            "Quantizing layer 6\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04970718541911229\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5449309867898418\n",
            "100% 7/7 [05:57<00:00, 51.08s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l6.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l6.pkl\n",
            "Quantizing layer 7\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04970076170610023\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5464573598278619\n",
            "100% 7/7 [05:35<00:00, 47.96s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l7.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l7.pkl\n",
            "Quantizing layer 8\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04979168195180943\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5102345362846098\n",
            "100% 7/7 [05:50<00:00, 50.07s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l8.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l8.pkl\n",
            "Quantizing layer 9\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.0497274448216888\n",
            "removing outliers by threshold\n",
            "p outlier: 0.46529473418398837\n",
            " 71% 5/7 [03:41<01:52, 56.40s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [06:09<00:00, 52.77s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l9.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l9.pkl\n",
            "Quantizing layer 10\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04977191668100307\n",
            "removing outliers by threshold\n",
            "p outlier: 0.5154184726853445\n",
            "100% 7/7 [05:27<00:00, 46.78s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l10.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l10.pkl\n",
            "Quantizing layer 11\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04959649990259674\n",
            "removing outliers by threshold\n",
            "p outlier: 0.584246581082517\n",
            "100% 7/7 [05:40<00:00, 48.66s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l11.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l11.pkl\n",
            "Quantizing layer 12\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04969680865193896\n",
            "removing outliers by threshold\n",
            "p outlier: 0.492712623714783\n",
            "100% 7/7 [05:26<00:00, 46.66s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l12.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l12.pkl\n",
            "Quantizing layer 13\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049604900142689444\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4779143654620709\n",
            " 71% 5/7 [03:23<01:41, 50.86s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:50<00:00, 50.04s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l13.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l13.pkl\n",
            "Quantizing layer 14\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049513485765210086\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4724715040137731\n",
            "100% 7/7 [05:30<00:00, 47.21s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l14.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l14.pkl\n",
            "Quantizing layer 15\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04983220075696243\n",
            "removing outliers by threshold\n",
            "p outlier: 0.44420074304768453\n",
            "100% 7/7 [05:45<00:00, 49.40s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l15.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l15.pkl\n",
            "Quantizing layer 16\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.0497704342856926\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4605085738582314\n",
            " 71% 5/7 [03:07<01:30, 45.37s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:29<00:00, 47.06s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l16.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l16.pkl\n",
            "Quantizing layer 17\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04961379451455229\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4064831708997025\n",
            " 71% 5/7 [03:25<01:33, 46.90s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:46<00:00, 49.53s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l17.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l17.pkl\n",
            "Quantizing layer 18\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049541651276109135\n",
            "removing outliers by threshold\n",
            "p outlier: 0.37808393567337273\n",
            " 71% 5/7 [03:14<01:32, 46.28s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:58<00:00, 51.27s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l18.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l18.pkl\n",
            "Quantizing layer 19\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04967457272228181\n",
            "removing outliers by threshold\n",
            "p outlier: 0.39431764671839575\n",
            "100% 7/7 [05:26<00:00, 46.62s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l19.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l19.pkl\n",
            "Quantizing layer 20\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04963405391712881\n",
            "removing outliers by threshold\n",
            "p outlier: 0.39396137771211137\n",
            "100% 7/7 [06:02<00:00, 51.73s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l20.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l20.pkl\n",
            "Quantizing layer 21\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049704220628491336\n",
            "removing outliers by threshold\n",
            "p outlier: 0.38834655840779836\n",
            " 71% 5/7 [03:18<01:35, 47.85s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:36<00:00, 48.08s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l21.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l21.pkl\n",
            "Quantizing layer 22\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04957278157762913\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3588127966371843\n",
            " 71% 5/7 [03:55<01:49, 54.75s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [06:18<00:00, 54.13s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l22.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l22.pkl\n",
            "Quantizing layer 23\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.0496676548774996\n",
            "removing outliers by threshold\n",
            "p outlier: 0.32626581933214255\n",
            "100% 7/7 [05:24<00:00, 46.31s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l23.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l23.pkl\n",
            "Quantizing layer 24\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049805517641373864\n",
            "removing outliers by threshold\n",
            "p outlier: 0.4045170205862411\n",
            " 57% 4/7 [01:58<01:25, 28.60s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:58<00:00, 51.21s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l24.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l24.pkl\n",
            "Quantizing layer 25\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049596994034366904\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3431488195231541\n",
            "100% 7/7 [06:10<00:00, 52.96s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l25.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l25.pkl\n",
            "Quantizing layer 26\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04951941534645199\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3972582248826101\n",
            "100% 7/7 [05:43<00:00, 49.10s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l26.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l26.pkl\n",
            "Quantizing layer 27\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04972596242637831\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3231552598389937\n",
            "  0% 0/7 [00:00<?, ?it/s]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [06:00<00:00, 51.57s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l27.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l27.pkl\n",
            "Quantizing layer 28\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.049877660879817036\n",
            "removing outliers by threshold\n",
            "p outlier: 0.36499586747717977\n",
            "  0% 0/7 [00:00<?, ?it/s]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            " 57% 4/7 [02:07<01:37, 32.35s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [06:01<00:00, 51.69s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l28.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l28.pkl\n",
            "Quantizing layer 29\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04963405391712881\n",
            "removing outliers by threshold\n",
            "p outlier: 0.413955431528042\n",
            "100% 7/7 [06:05<00:00, 52.28s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l29.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l29.pkl\n",
            "Quantizing layer 30\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.0498361538111237\n",
            "removing outliers by threshold\n",
            "p outlier: 0.3970655134922482\n",
            " 57% 4/7 [01:56<01:26, 28.75s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            " 71% 5/7 [03:14<01:33, 46.65s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [06:04<00:00, 52.08s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l30.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l30.pkl\n",
            "Quantizing layer 31\n",
            "Removing outliers outlier=0.53, sensitivity=0.05\n",
            "removing outliers by sensitivity\n",
            "p outlier: 0.04955746349275421\n",
            "removing outliers by threshold\n",
            "p outlier: 0.47061752161213766\n",
            " 57% 4/7 [01:53<01:24, 28.19s/it]/content/SqueezeLLM/quantization/nuq.py:166: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
            "  kmeans = KMeans(\n",
            "100% 7/7 [05:45<00:00, 49.42s/it]\n",
            "Saving layer lut to drive/MyDrive/llama-LUT-3bit//lut/l31.pkl\n",
            "Saving layer outliers to drive/MyDrive/llama-LUT-3bit//outliers/l31.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python SqueezeLLM/quantization/pack.py --model meta-llama/Llama-2-7b-hf --wbits 3 --folder drive/MyDrive/llama-LUT-3bit/ --save drive/MyDrive/llama-squeezed-3bit --include_sparse --balance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP04tIAPyt_6",
        "outputId": "0ed3142e-7afa-4a73-eec8-43702d0b9e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint shards: 100% 2/2 [00:26<00:00, 13.09s/it]\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 756kB/s]\n",
            "Running llama_sequential\n",
            "Starting ...\n",
            "llama_sequential Done: 202.52166318893433\n",
            "Running llama_pack\n",
            "Packing ...\n",
            "model.layers.0.self_attn.k_proj\n",
            "/content/SqueezeLLM/squeezellm/quant.py:126: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  outliers = outliers.to_sparse(layout=torch.sparse_csr)\n",
            "self.numvals:  970119\n",
            "self.rows:  4097\n",
            "self.num_threads:  97024\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.self_attn.o_proj\n",
            "self.numvals:  645210\n",
            "self.rows:  4097\n",
            "self.num_threads:  64640\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.self_attn.q_proj\n",
            "self.numvals:  1189179\n",
            "self.rows:  4097\n",
            "self.num_threads:  119040\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.self_attn.v_proj\n",
            "self.numvals:  454257\n",
            "self.rows:  4097\n",
            "self.num_threads:  45440\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.mlp.down_proj\n",
            "self.numvals:  152392\n",
            "self.rows:  4097\n",
            "self.num_threads:  15360\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.mlp.gate_proj\n",
            "self.numvals:  171929\n",
            "self.rows:  11009\n",
            "self.num_threads:  17280\n",
            "nnz_per_thread:  10\n",
            "model.layers.0.mlp.up_proj\n",
            "self.numvals:  141932\n",
            "self.rows:  11009\n",
            "self.num_threads:  14208\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.self_attn.k_proj\n",
            "self.numvals:  922543\n",
            "self.rows:  4097\n",
            "self.num_threads:  92288\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.self_attn.o_proj\n",
            "self.numvals:  795880\n",
            "self.rows:  4097\n",
            "self.num_threads:  79616\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.self_attn.q_proj\n",
            "self.numvals:  1094630\n",
            "self.rows:  4097\n",
            "self.num_threads:  109568\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.self_attn.v_proj\n",
            "self.numvals:  433267\n",
            "self.rows:  4097\n",
            "self.num_threads:  43392\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.mlp.down_proj\n",
            "self.numvals:  125890\n",
            "self.rows:  4097\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.mlp.gate_proj\n",
            "self.numvals:  148839\n",
            "self.rows:  11009\n",
            "self.num_threads:  14976\n",
            "nnz_per_thread:  10\n",
            "model.layers.1.mlp.up_proj\n",
            "self.numvals:  124182\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.self_attn.k_proj\n",
            "self.numvals:  663813\n",
            "self.rows:  4097\n",
            "self.num_threads:  66432\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.self_attn.o_proj\n",
            "self.numvals:  63959\n",
            "self.rows:  4097\n",
            "self.num_threads:  6400\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.self_attn.q_proj\n",
            "self.numvals:  488217\n",
            "self.rows:  4097\n",
            "self.num_threads:  48896\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.self_attn.v_proj\n",
            "self.numvals:  62655\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.mlp.down_proj\n",
            "self.numvals:  116601\n",
            "self.rows:  4097\n",
            "self.num_threads:  11776\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.mlp.gate_proj\n",
            "self.numvals:  125550\n",
            "self.rows:  11009\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.2.mlp.up_proj\n",
            "self.numvals:  119211\n",
            "self.rows:  11009\n",
            "self.num_threads:  12032\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.self_attn.k_proj\n",
            "self.numvals:  387213\n",
            "self.rows:  4097\n",
            "self.num_threads:  38784\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.self_attn.o_proj\n",
            "self.numvals:  69683\n",
            "self.rows:  4097\n",
            "self.num_threads:  7040\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.self_attn.q_proj\n",
            "self.numvals:  308852\n",
            "self.rows:  4097\n",
            "self.num_threads:  30976\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.self_attn.v_proj\n",
            "self.numvals:  70757\n",
            "self.rows:  4097\n",
            "self.num_threads:  7168\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.mlp.down_proj\n",
            "self.numvals:  116988\n",
            "self.rows:  4097\n",
            "self.num_threads:  11776\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.mlp.gate_proj\n",
            "self.numvals:  131833\n",
            "self.rows:  11009\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.3.mlp.up_proj\n",
            "self.numvals:  124096\n",
            "self.rows:  11009\n",
            "self.num_threads:  12416\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.self_attn.k_proj\n",
            "self.numvals:  308518\n",
            "self.rows:  4097\n",
            "self.num_threads:  30976\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.self_attn.o_proj\n",
            "self.numvals:  55642\n",
            "self.rows:  4097\n",
            "self.num_threads:  5632\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.self_attn.q_proj\n",
            "self.numvals:  238812\n",
            "self.rows:  4097\n",
            "self.num_threads:  23936\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.self_attn.v_proj\n",
            "self.numvals:  57128\n",
            "self.rows:  4097\n",
            "self.num_threads:  5760\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.mlp.down_proj\n",
            "self.numvals:  125460\n",
            "self.rows:  4097\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.mlp.gate_proj\n",
            "self.numvals:  158667\n",
            "self.rows:  11009\n",
            "self.num_threads:  15872\n",
            "nnz_per_thread:  10\n",
            "model.layers.4.mlp.up_proj\n",
            "self.numvals:  122864\n",
            "self.rows:  11009\n",
            "self.num_threads:  12288\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.self_attn.k_proj\n",
            "self.numvals:  263471\n",
            "self.rows:  4097\n",
            "self.num_threads:  26368\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.self_attn.o_proj\n",
            "self.numvals:  57778\n",
            "self.rows:  4097\n",
            "self.num_threads:  5888\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.self_attn.q_proj\n",
            "self.numvals:  199854\n",
            "self.rows:  4097\n",
            "self.num_threads:  20096\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.self_attn.v_proj\n",
            "self.numvals:  65777\n",
            "self.rows:  4097\n",
            "self.num_threads:  6656\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.mlp.down_proj\n",
            "self.numvals:  127029\n",
            "self.rows:  4097\n",
            "self.num_threads:  12800\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.mlp.gate_proj\n",
            "self.numvals:  180681\n",
            "self.rows:  11009\n",
            "self.num_threads:  18176\n",
            "nnz_per_thread:  10\n",
            "model.layers.5.mlp.up_proj\n",
            "self.numvals:  124472\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.self_attn.k_proj\n",
            "self.numvals:  325098\n",
            "self.rows:  4097\n",
            "self.num_threads:  32512\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.self_attn.o_proj\n",
            "self.numvals:  69741\n",
            "self.rows:  4097\n",
            "self.num_threads:  7040\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.self_attn.q_proj\n",
            "self.numvals:  280881\n",
            "self.rows:  4097\n",
            "self.num_threads:  28160\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.self_attn.v_proj\n",
            "self.numvals:  81619\n",
            "self.rows:  4097\n",
            "self.num_threads:  8192\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.mlp.down_proj\n",
            "self.numvals:  125220\n",
            "self.rows:  4097\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.mlp.gate_proj\n",
            "self.numvals:  195634\n",
            "self.rows:  11009\n",
            "self.num_threads:  19584\n",
            "nnz_per_thread:  10\n",
            "model.layers.6.mlp.up_proj\n",
            "self.numvals:  125206\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.self_attn.k_proj\n",
            "self.numvals:  305032\n",
            "self.rows:  4097\n",
            "self.num_threads:  30592\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.self_attn.o_proj\n",
            "self.numvals:  77850\n",
            "self.rows:  4097\n",
            "self.num_threads:  7808\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.self_attn.q_proj\n",
            "self.numvals:  281158\n",
            "self.rows:  4097\n",
            "self.num_threads:  28160\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.self_attn.v_proj\n",
            "self.numvals:  86367\n",
            "self.rows:  4097\n",
            "self.num_threads:  8704\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.mlp.down_proj\n",
            "self.numvals:  124374\n",
            "self.rows:  4097\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.mlp.gate_proj\n",
            "self.numvals:  206517\n",
            "self.rows:  11009\n",
            "self.num_threads:  20736\n",
            "nnz_per_thread:  10\n",
            "model.layers.7.mlp.up_proj\n",
            "self.numvals:  125178\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.self_attn.k_proj\n",
            "self.numvals:  278923\n",
            "self.rows:  4097\n",
            "self.num_threads:  27904\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.self_attn.o_proj\n",
            "self.numvals:  60150\n",
            "self.rows:  4097\n",
            "self.num_threads:  6016\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.self_attn.q_proj\n",
            "self.numvals:  255153\n",
            "self.rows:  4097\n",
            "self.num_threads:  25600\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.self_attn.v_proj\n",
            "self.numvals:  71661\n",
            "self.rows:  4097\n",
            "self.num_threads:  7168\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.mlp.down_proj\n",
            "self.numvals:  131950\n",
            "self.rows:  4097\n",
            "self.num_threads:  13312\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.mlp.gate_proj\n",
            "self.numvals:  210313\n",
            "self.rows:  11009\n",
            "self.num_threads:  21120\n",
            "nnz_per_thread:  10\n",
            "model.layers.8.mlp.up_proj\n",
            "self.numvals:  125204\n",
            "self.rows:  11009\n",
            "self.num_threads:  12544\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.self_attn.k_proj\n",
            "self.numvals:  235224\n",
            "self.rows:  4097\n",
            "self.num_threads:  23552\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.self_attn.o_proj\n",
            "self.numvals:  60972\n",
            "self.rows:  4097\n",
            "self.num_threads:  6144\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.self_attn.q_proj\n",
            "self.numvals:  192528\n",
            "self.rows:  4097\n",
            "self.num_threads:  19328\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.self_attn.v_proj\n",
            "self.numvals:  72562\n",
            "self.rows:  4097\n",
            "self.num_threads:  7296\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.mlp.down_proj\n",
            "self.numvals:  126979\n",
            "self.rows:  4097\n",
            "self.num_threads:  12800\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.mlp.gate_proj\n",
            "self.numvals:  224394\n",
            "self.rows:  11009\n",
            "self.num_threads:  22528\n",
            "nnz_per_thread:  10\n",
            "model.layers.9.mlp.up_proj\n",
            "self.numvals:  129618\n",
            "self.rows:  11009\n",
            "self.num_threads:  13056\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.self_attn.k_proj\n",
            "self.numvals:  278686\n",
            "self.rows:  4097\n",
            "self.num_threads:  27904\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.self_attn.o_proj\n",
            "self.numvals:  56413\n",
            "self.rows:  4097\n",
            "self.num_threads:  5760\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.self_attn.q_proj\n",
            "self.numvals:  217730\n",
            "self.rows:  4097\n",
            "self.num_threads:  21888\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.self_attn.v_proj\n",
            "self.numvals:  65064\n",
            "self.rows:  4097\n",
            "self.num_threads:  6528\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.mlp.down_proj\n",
            "self.numvals:  133059\n",
            "self.rows:  4097\n",
            "self.num_threads:  13312\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.mlp.gate_proj\n",
            "self.numvals:  258714\n",
            "self.rows:  11009\n",
            "self.num_threads:  25984\n",
            "nnz_per_thread:  10\n",
            "model.layers.10.mlp.up_proj\n",
            "self.numvals:  134139\n",
            "self.rows:  11009\n",
            "self.num_threads:  13440\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.self_attn.k_proj\n",
            "self.numvals:  327886\n",
            "self.rows:  4097\n",
            "self.num_threads:  32896\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.self_attn.o_proj\n",
            "self.numvals:  70259\n",
            "self.rows:  4097\n",
            "self.num_threads:  7040\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.self_attn.q_proj\n",
            "self.numvals:  312503\n",
            "self.rows:  4097\n",
            "self.num_threads:  31360\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.self_attn.v_proj\n",
            "self.numvals:  73444\n",
            "self.rows:  4097\n",
            "self.num_threads:  7424\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.mlp.down_proj\n",
            "self.numvals:  129076\n",
            "self.rows:  4097\n",
            "self.num_threads:  12928\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.mlp.gate_proj\n",
            "self.numvals:  236844\n",
            "self.rows:  11009\n",
            "self.num_threads:  23808\n",
            "nnz_per_thread:  10\n",
            "model.layers.11.mlp.up_proj\n",
            "self.numvals:  132729\n",
            "self.rows:  11009\n",
            "self.num_threads:  13312\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.self_attn.k_proj\n",
            "self.numvals:  258005\n",
            "self.rows:  4097\n",
            "self.num_threads:  25856\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.self_attn.o_proj\n",
            "self.numvals:  65754\n",
            "self.rows:  4097\n",
            "self.num_threads:  6656\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.self_attn.q_proj\n",
            "self.numvals:  211605\n",
            "self.rows:  4097\n",
            "self.num_threads:  21248\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.self_attn.v_proj\n",
            "self.numvals:  75588\n",
            "self.rows:  4097\n",
            "self.num_threads:  7680\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.mlp.down_proj\n",
            "self.numvals:  133450\n",
            "self.rows:  4097\n",
            "self.num_threads:  13440\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.mlp.gate_proj\n",
            "self.numvals:  225138\n",
            "self.rows:  11009\n",
            "self.num_threads:  22528\n",
            "nnz_per_thread:  10\n",
            "model.layers.12.mlp.up_proj\n",
            "self.numvals:  128162\n",
            "self.rows:  11009\n",
            "self.num_threads:  12928\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.self_attn.k_proj\n",
            "self.numvals:  238148\n",
            "self.rows:  4097\n",
            "self.num_threads:  23936\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.self_attn.o_proj\n",
            "self.numvals:  62570\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.self_attn.q_proj\n",
            "self.numvals:  204668\n",
            "self.rows:  4097\n",
            "self.num_threads:  20480\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.self_attn.v_proj\n",
            "self.numvals:  71520\n",
            "self.rows:  4097\n",
            "self.num_threads:  7168\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.mlp.down_proj\n",
            "self.numvals:  133344\n",
            "self.rows:  4097\n",
            "self.num_threads:  13440\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.mlp.gate_proj\n",
            "self.numvals:  226285\n",
            "self.rows:  11009\n",
            "self.num_threads:  22656\n",
            "nnz_per_thread:  10\n",
            "model.layers.13.mlp.up_proj\n",
            "self.numvals:  131032\n",
            "self.rows:  11009\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.self_attn.k_proj\n",
            "self.numvals:  241345\n",
            "self.rows:  4097\n",
            "self.num_threads:  24192\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.self_attn.o_proj\n",
            "self.numvals:  56949\n",
            "self.rows:  4097\n",
            "self.num_threads:  5760\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.self_attn.q_proj\n",
            "self.numvals:  214512\n",
            "self.rows:  4097\n",
            "self.num_threads:  21504\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.self_attn.v_proj\n",
            "self.numvals:  64784\n",
            "self.rows:  4097\n",
            "self.num_threads:  6528\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.mlp.down_proj\n",
            "self.numvals:  133018\n",
            "self.rows:  4097\n",
            "self.num_threads:  13312\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.mlp.gate_proj\n",
            "self.numvals:  214459\n",
            "self.rows:  11009\n",
            "self.num_threads:  21504\n",
            "nnz_per_thread:  10\n",
            "model.layers.14.mlp.up_proj\n",
            "self.numvals:  131301\n",
            "self.rows:  11009\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.self_attn.k_proj\n",
            "self.numvals:  214606\n",
            "self.rows:  4097\n",
            "self.num_threads:  21504\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.self_attn.o_proj\n",
            "self.numvals:  56942\n",
            "self.rows:  4097\n",
            "self.num_threads:  5760\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.self_attn.q_proj\n",
            "self.numvals:  176288\n",
            "self.rows:  4097\n",
            "self.num_threads:  17664\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.self_attn.v_proj\n",
            "self.numvals:  60205\n",
            "self.rows:  4097\n",
            "self.num_threads:  6144\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.mlp.down_proj\n",
            "self.numvals:  129580\n",
            "self.rows:  4097\n",
            "self.num_threads:  13056\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.mlp.gate_proj\n",
            "self.numvals:  227875\n",
            "self.rows:  11009\n",
            "self.num_threads:  22912\n",
            "nnz_per_thread:  10\n",
            "model.layers.15.mlp.up_proj\n",
            "self.numvals:  134303\n",
            "self.rows:  11009\n",
            "self.num_threads:  13440\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.self_attn.k_proj\n",
            "self.numvals:  242480\n",
            "self.rows:  4097\n",
            "self.num_threads:  24320\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.self_attn.o_proj\n",
            "self.numvals:  55256\n",
            "self.rows:  4097\n",
            "self.num_threads:  5632\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.self_attn.q_proj\n",
            "self.numvals:  203860\n",
            "self.rows:  4097\n",
            "self.num_threads:  20480\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.self_attn.v_proj\n",
            "self.numvals:  61546\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.mlp.down_proj\n",
            "self.numvals:  125586\n",
            "self.rows:  4097\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.mlp.gate_proj\n",
            "self.numvals:  213571\n",
            "self.rows:  11009\n",
            "self.num_threads:  21376\n",
            "nnz_per_thread:  10\n",
            "model.layers.16.mlp.up_proj\n",
            "self.numvals:  130379\n",
            "self.rows:  11009\n",
            "self.num_threads:  13056\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.self_attn.k_proj\n",
            "self.numvals:  196730\n",
            "self.rows:  4097\n",
            "self.num_threads:  19712\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.self_attn.o_proj\n",
            "self.numvals:  54714\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.self_attn.q_proj\n",
            "self.numvals:  161350\n",
            "self.rows:  4097\n",
            "self.num_threads:  16256\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.self_attn.v_proj\n",
            "self.numvals:  56201\n",
            "self.rows:  4097\n",
            "self.num_threads:  5632\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.mlp.down_proj\n",
            "self.numvals:  121513\n",
            "self.rows:  4097\n",
            "self.num_threads:  12160\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.mlp.gate_proj\n",
            "self.numvals:  209486\n",
            "self.rows:  11009\n",
            "self.num_threads:  20992\n",
            "nnz_per_thread:  10\n",
            "model.layers.17.mlp.up_proj\n",
            "self.numvals:  123033\n",
            "self.rows:  11009\n",
            "self.num_threads:  12416\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.self_attn.k_proj\n",
            "self.numvals:  169804\n",
            "self.rows:  4097\n",
            "self.num_threads:  17024\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.self_attn.o_proj\n",
            "self.numvals:  53395\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.self_attn.q_proj\n",
            "self.numvals:  142135\n",
            "self.rows:  4097\n",
            "self.num_threads:  14336\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.self_attn.v_proj\n",
            "self.numvals:  58886\n",
            "self.rows:  4097\n",
            "self.num_threads:  6016\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.mlp.down_proj\n",
            "self.numvals:  118938\n",
            "self.rows:  4097\n",
            "self.num_threads:  11904\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.mlp.gate_proj\n",
            "self.numvals:  199523\n",
            "self.rows:  11009\n",
            "self.num_threads:  19968\n",
            "nnz_per_thread:  10\n",
            "model.layers.18.mlp.up_proj\n",
            "self.numvals:  122727\n",
            "self.rows:  11009\n",
            "self.num_threads:  12288\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.self_attn.k_proj\n",
            "self.numvals:  193156\n",
            "self.rows:  4097\n",
            "self.num_threads:  19328\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.self_attn.o_proj\n",
            "self.numvals:  55666\n",
            "self.rows:  4097\n",
            "self.num_threads:  5632\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.self_attn.q_proj\n",
            "self.numvals:  160467\n",
            "self.rows:  4097\n",
            "self.num_threads:  16128\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.self_attn.v_proj\n",
            "self.numvals:  62282\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.mlp.down_proj\n",
            "self.numvals:  120820\n",
            "self.rows:  4097\n",
            "self.num_threads:  12160\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.mlp.gate_proj\n",
            "self.numvals:  184897\n",
            "self.rows:  11009\n",
            "self.num_threads:  18560\n",
            "nnz_per_thread:  10\n",
            "model.layers.19.mlp.up_proj\n",
            "self.numvals:  121242\n",
            "self.rows:  11009\n",
            "self.num_threads:  12160\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.self_attn.k_proj\n",
            "self.numvals:  211208\n",
            "self.rows:  4097\n",
            "self.num_threads:  21248\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.self_attn.o_proj\n",
            "self.numvals:  49512\n",
            "self.rows:  4097\n",
            "self.num_threads:  4992\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.self_attn.q_proj\n",
            "self.numvals:  178956\n",
            "self.rows:  4097\n",
            "self.num_threads:  17920\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.self_attn.v_proj\n",
            "self.numvals:  50096\n",
            "self.rows:  4097\n",
            "self.num_threads:  5120\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.mlp.down_proj\n",
            "self.numvals:  118973\n",
            "self.rows:  4097\n",
            "self.num_threads:  11904\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.mlp.gate_proj\n",
            "self.numvals:  169894\n",
            "self.rows:  11009\n",
            "self.num_threads:  17024\n",
            "nnz_per_thread:  10\n",
            "model.layers.20.mlp.up_proj\n",
            "self.numvals:  119086\n",
            "self.rows:  11009\n",
            "self.num_threads:  12032\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.self_attn.k_proj\n",
            "self.numvals:  214202\n",
            "self.rows:  4097\n",
            "self.num_threads:  21504\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.self_attn.o_proj\n",
            "self.numvals:  47686\n",
            "self.rows:  4097\n",
            "self.num_threads:  4864\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.self_attn.q_proj\n",
            "self.numvals:  177748\n",
            "self.rows:  4097\n",
            "self.num_threads:  17792\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.self_attn.v_proj\n",
            "self.numvals:  52825\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.mlp.down_proj\n",
            "self.numvals:  113816\n",
            "self.rows:  4097\n",
            "self.num_threads:  11392\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.mlp.gate_proj\n",
            "self.numvals:  163179\n",
            "self.rows:  11009\n",
            "self.num_threads:  16384\n",
            "nnz_per_thread:  10\n",
            "model.layers.21.mlp.up_proj\n",
            "self.numvals:  117050\n",
            "self.rows:  11009\n",
            "self.num_threads:  11776\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.self_attn.k_proj\n",
            "self.numvals:  177105\n",
            "self.rows:  4097\n",
            "self.num_threads:  17792\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.self_attn.o_proj\n",
            "self.numvals:  54364\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.self_attn.q_proj\n",
            "self.numvals:  156110\n",
            "self.rows:  4097\n",
            "self.num_threads:  15616\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.self_attn.v_proj\n",
            "self.numvals:  53736\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.mlp.down_proj\n",
            "self.numvals:  113762\n",
            "self.rows:  4097\n",
            "self.num_threads:  11392\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.mlp.gate_proj\n",
            "self.numvals:  156434\n",
            "self.rows:  11009\n",
            "self.num_threads:  15744\n",
            "nnz_per_thread:  10\n",
            "model.layers.22.mlp.up_proj\n",
            "self.numvals:  114960\n",
            "self.rows:  11009\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.self_attn.k_proj\n",
            "self.numvals:  144807\n",
            "self.rows:  4097\n",
            "self.num_threads:  14592\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.self_attn.o_proj\n",
            "self.numvals:  53112\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.self_attn.q_proj\n",
            "self.numvals:  131461\n",
            "self.rows:  4097\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.self_attn.v_proj\n",
            "self.numvals:  54433\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.mlp.down_proj\n",
            "self.numvals:  114917\n",
            "self.rows:  4097\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.mlp.gate_proj\n",
            "self.numvals:  147160\n",
            "self.rows:  11009\n",
            "self.num_threads:  14720\n",
            "nnz_per_thread:  10\n",
            "model.layers.23.mlp.up_proj\n",
            "self.numvals:  114906\n",
            "self.rows:  11009\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.self_attn.k_proj\n",
            "self.numvals:  235696\n",
            "self.rows:  4097\n",
            "self.num_threads:  23680\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.self_attn.o_proj\n",
            "self.numvals:  49641\n",
            "self.rows:  4097\n",
            "self.num_threads:  4992\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.self_attn.q_proj\n",
            "self.numvals:  205627\n",
            "self.rows:  4097\n",
            "self.num_threads:  20608\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.self_attn.v_proj\n",
            "self.numvals:  53212\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.mlp.down_proj\n",
            "self.numvals:  113112\n",
            "self.rows:  4097\n",
            "self.num_threads:  11392\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.mlp.gate_proj\n",
            "self.numvals:  148071\n",
            "self.rows:  11009\n",
            "self.num_threads:  14848\n",
            "nnz_per_thread:  10\n",
            "model.layers.24.mlp.up_proj\n",
            "self.numvals:  114077\n",
            "self.rows:  11009\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.self_attn.k_proj\n",
            "self.numvals:  158391\n",
            "self.rows:  4097\n",
            "self.num_threads:  15872\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.self_attn.o_proj\n",
            "self.numvals:  48859\n",
            "self.rows:  4097\n",
            "self.num_threads:  4992\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.self_attn.q_proj\n",
            "self.numvals:  151578\n",
            "self.rows:  4097\n",
            "self.num_threads:  15232\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.self_attn.v_proj\n",
            "self.numvals:  52518\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.mlp.down_proj\n",
            "self.numvals:  114832\n",
            "self.rows:  4097\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.mlp.gate_proj\n",
            "self.numvals:  154988\n",
            "self.rows:  11009\n",
            "self.num_threads:  15616\n",
            "nnz_per_thread:  10\n",
            "model.layers.25.mlp.up_proj\n",
            "self.numvals:  113654\n",
            "self.rows:  11009\n",
            "self.num_threads:  11392\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.self_attn.k_proj\n",
            "self.numvals:  182032\n",
            "self.rows:  4097\n",
            "self.num_threads:  18304\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.self_attn.o_proj\n",
            "self.numvals:  74384\n",
            "self.rows:  4097\n",
            "self.num_threads:  7552\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.self_attn.q_proj\n",
            "self.numvals:  167716\n",
            "self.rows:  4097\n",
            "self.num_threads:  16896\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.self_attn.v_proj\n",
            "self.numvals:  62451\n",
            "self.rows:  4097\n",
            "self.num_threads:  6272\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.mlp.down_proj\n",
            "self.numvals:  118607\n",
            "self.rows:  4097\n",
            "self.num_threads:  11904\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.mlp.gate_proj\n",
            "self.numvals:  180747\n",
            "self.rows:  11009\n",
            "self.num_threads:  18176\n",
            "nnz_per_thread:  10\n",
            "model.layers.26.mlp.up_proj\n",
            "self.numvals:  118230\n",
            "self.rows:  11009\n",
            "self.num_threads:  11904\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.self_attn.k_proj\n",
            "self.numvals:  109389\n",
            "self.rows:  4097\n",
            "self.num_threads:  11008\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.self_attn.o_proj\n",
            "self.numvals:  53485\n",
            "self.rows:  4097\n",
            "self.num_threads:  5376\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.self_attn.q_proj\n",
            "self.numvals:  104370\n",
            "self.rows:  4097\n",
            "self.num_threads:  10496\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.self_attn.v_proj\n",
            "self.numvals:  52381\n",
            "self.rows:  4097\n",
            "self.num_threads:  5248\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.mlp.down_proj\n",
            "self.numvals:  121148\n",
            "self.rows:  4097\n",
            "self.num_threads:  12160\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.mlp.gate_proj\n",
            "self.numvals:  198006\n",
            "self.rows:  11009\n",
            "self.num_threads:  19840\n",
            "nnz_per_thread:  10\n",
            "model.layers.27.mlp.up_proj\n",
            "self.numvals:  115840\n",
            "self.rows:  11009\n",
            "self.num_threads:  11648\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.self_attn.k_proj\n",
            "self.numvals:  135169\n",
            "self.rows:  4097\n",
            "self.num_threads:  13568\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.self_attn.o_proj\n",
            "self.numvals:  58035\n",
            "self.rows:  4097\n",
            "self.num_threads:  5888\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.self_attn.q_proj\n",
            "self.numvals:  127043\n",
            "self.rows:  4097\n",
            "self.num_threads:  12800\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.self_attn.v_proj\n",
            "self.numvals:  61233\n",
            "self.rows:  4097\n",
            "self.num_threads:  6144\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.mlp.down_proj\n",
            "self.numvals:  122111\n",
            "self.rows:  4097\n",
            "self.num_threads:  12288\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.mlp.gate_proj\n",
            "self.numvals:  216555\n",
            "self.rows:  11009\n",
            "self.num_threads:  21760\n",
            "nnz_per_thread:  10\n",
            "model.layers.28.mlp.up_proj\n",
            "self.numvals:  119455\n",
            "self.rows:  11009\n",
            "self.num_threads:  12032\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.self_attn.k_proj\n",
            "self.numvals:  173976\n",
            "self.rows:  4097\n",
            "self.num_threads:  17408\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.self_attn.o_proj\n",
            "self.numvals:  53784\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.self_attn.q_proj\n",
            "self.numvals:  157795\n",
            "self.rows:  4097\n",
            "self.num_threads:  15872\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.self_attn.v_proj\n",
            "self.numvals:  54955\n",
            "self.rows:  4097\n",
            "self.num_threads:  5504\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.mlp.down_proj\n",
            "self.numvals:  130820\n",
            "self.rows:  4097\n",
            "self.num_threads:  13184\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.mlp.gate_proj\n",
            "self.numvals:  244054\n",
            "self.rows:  11009\n",
            "self.num_threads:  24448\n",
            "nnz_per_thread:  10\n",
            "model.layers.29.mlp.up_proj\n",
            "self.numvals:  122806\n",
            "self.rows:  11009\n",
            "self.num_threads:  12288\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.self_attn.k_proj\n",
            "self.numvals:  123407\n",
            "self.rows:  4097\n",
            "self.num_threads:  12416\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.self_attn.o_proj\n",
            "self.numvals:  52056\n",
            "self.rows:  4097\n",
            "self.num_threads:  5248\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.self_attn.q_proj\n",
            "self.numvals:  114708\n",
            "self.rows:  4097\n",
            "self.num_threads:  11520\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.self_attn.v_proj\n",
            "self.numvals:  51980\n",
            "self.rows:  4097\n",
            "self.num_threads:  5248\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.mlp.down_proj\n",
            "self.numvals:  156875\n",
            "self.rows:  4097\n",
            "self.num_threads:  15744\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.mlp.gate_proj\n",
            "self.numvals:  270572\n",
            "self.rows:  11009\n",
            "self.num_threads:  27136\n",
            "nnz_per_thread:  10\n",
            "model.layers.30.mlp.up_proj\n",
            "self.numvals:  134820\n",
            "self.rows:  11009\n",
            "self.num_threads:  13568\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.self_attn.k_proj\n",
            "self.numvals:  159125\n",
            "self.rows:  4097\n",
            "self.num_threads:  16000\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.self_attn.o_proj\n",
            "self.numvals:  71317\n",
            "self.rows:  4097\n",
            "self.num_threads:  7168\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.self_attn.q_proj\n",
            "self.numvals:  125949\n",
            "self.rows:  4097\n",
            "self.num_threads:  12672\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.self_attn.v_proj\n",
            "self.numvals:  66123\n",
            "self.rows:  4097\n",
            "self.num_threads:  6656\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.mlp.down_proj\n",
            "self.numvals:  195508\n",
            "self.rows:  4097\n",
            "self.num_threads:  19584\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.mlp.gate_proj\n",
            "self.numvals:  266385\n",
            "self.rows:  11009\n",
            "self.num_threads:  26752\n",
            "nnz_per_thread:  10\n",
            "model.layers.31.mlp.up_proj\n",
            "self.numvals:  168298\n",
            "self.rows:  11009\n",
            "self.num_threads:  16896\n",
            "nnz_per_thread:  10\n",
            "Done.\n",
            "llama_pack Done: 822.726713180542\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eee3d51eba8948d2a8b7e997378409a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b022bb73c7c4fd58082eda3445cfab1",
              "IPY_MODEL_9a521189b23b4e5c8ed359d61fb13bc1",
              "IPY_MODEL_d0bbe402729f47bebe07db37d3553aac",
              "IPY_MODEL_e632dd933c3d482dae36b8ce5a75a7c1"
            ],
            "layout": "IPY_MODEL_96a0b31e5217441d975ef5b066094bd6"
          }
        },
        "f1c0d8ccd54e4f86a720115a3df2c805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e36e38a9e4141ff93125c54859d7eab",
            "placeholder": "​",
            "style": "IPY_MODEL_dac2f0a09a744be89c8db02916a34bde",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a2fff39d908342d6b39dec223ececb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_bdfce33dd9ee44ea878fcb8df0c3fd01",
            "placeholder": "​",
            "style": "IPY_MODEL_05d17a2bcf4045f1b143349336c15ef1",
            "value": ""
          }
        },
        "a9281ca81d7b4148874867c73cea14ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_816a602e65644e128df40bf1f549e418",
            "style": "IPY_MODEL_a9b812b80cc54c7aa050ee810756c339",
            "value": true
          }
        },
        "59f22d232fec465ea3e9843d9eaafee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b112918b61c741819bb138de0bc20565",
            "style": "IPY_MODEL_bc0d8fa2739b4bcb9539b63ba6f5bcc7",
            "tooltip": ""
          }
        },
        "063abb89c097488fbb87b4c2175bff8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_746a2281325f4541951d2312278889e5",
            "placeholder": "​",
            "style": "IPY_MODEL_57d6b3715b3b4e1988cf8976f33ebc37",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "96a0b31e5217441d975ef5b066094bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1e36e38a9e4141ff93125c54859d7eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dac2f0a09a744be89c8db02916a34bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdfce33dd9ee44ea878fcb8df0c3fd01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d17a2bcf4045f1b143349336c15ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "816a602e65644e128df40bf1f549e418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b812b80cc54c7aa050ee810756c339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b112918b61c741819bb138de0bc20565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0d8fa2739b4bcb9539b63ba6f5bcc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "746a2281325f4541951d2312278889e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57d6b3715b3b4e1988cf8976f33ebc37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b8bf43b1a241e598c939bde73fb552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d186ead076463294f0e04be06c4cd9",
            "placeholder": "​",
            "style": "IPY_MODEL_fbe4cb5033a14a578d98ed3ec1ee0479",
            "value": "Connecting..."
          }
        },
        "06d186ead076463294f0e04be06c4cd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe4cb5033a14a578d98ed3ec1ee0479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b022bb73c7c4fd58082eda3445cfab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa95fb9a1696497cb9592c0f787aa23e",
            "placeholder": "​",
            "style": "IPY_MODEL_17afd077c02e458a821562bcf8e592cc",
            "value": "Token is valid (permission: read)."
          }
        },
        "9a521189b23b4e5c8ed359d61fb13bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae0a86d49094b9cbd4a0027b0e5cbd7",
            "placeholder": "​",
            "style": "IPY_MODEL_4656b587da364bbcb6bbc5bd1ab00252",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "d0bbe402729f47bebe07db37d3553aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b4ccc60c6374ae5b1bb190c5d5f6594",
            "placeholder": "​",
            "style": "IPY_MODEL_d3b7e58a709c41cb96c6ee51e12006e9",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "e632dd933c3d482dae36b8ce5a75a7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c59ca20bcbb45fe830ccef5316c2812",
            "placeholder": "​",
            "style": "IPY_MODEL_c9c42488223d4536af899f52c5ff7e4a",
            "value": "Login successful"
          }
        },
        "aa95fb9a1696497cb9592c0f787aa23e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17afd077c02e458a821562bcf8e592cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bae0a86d49094b9cbd4a0027b0e5cbd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4656b587da364bbcb6bbc5bd1ab00252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b4ccc60c6374ae5b1bb190c5d5f6594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3b7e58a709c41cb96c6ee51e12006e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c59ca20bcbb45fe830ccef5316c2812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9c42488223d4536af899f52c5ff7e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}