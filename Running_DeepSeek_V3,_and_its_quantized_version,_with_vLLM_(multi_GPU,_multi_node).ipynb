{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptagundlapalli/Applied_Data_Analytics/blob/master/Running_DeepSeek_V3%2C_and_its_quantized_version%2C_with_vLLM_(multi_GPU%2C_multi_node).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "zuXdojB-pwqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "-tUlvIJj5E1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mkgY6MQiW2m"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade vllm triton"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*More details in this article: [DeepSeek-V3: Running the Best Open LLM Locally](https://kaitchup.substack.com/p/deepseek-v3-running-the-best-open)*\n",
        "\n",
        "This notebook shows how to run DeepSeek-V3 with vLLM. Examples of configurations that can run DeepSeek-V3:\n",
        "\n",
        "* 8 H200s\n",
        "* 2 nodes of 8 H100s"
      ],
      "metadata": {
        "id": "rKjE385Gqab8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With 1 node of 8 GPUs"
      ],
      "metadata": {
        "id": "AYEBXRsIiuQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "prompts = [{\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"},\n",
        "          {\"role\":\"user\", \"content\":\"Please add a pair of parentheses to the incorrect equation: 1 + 2 * 3 + 4 * 5 + 6 * 7 + 8 * 9 = 479, to make the equation true.\"}]\n",
        "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=5000)\n",
        "\n",
        "llm = LLM(model=\"deepseek-ai/DeepSeek-V3\", max_model_len=8192, tensor_parallel_size=8)\n",
        "\n",
        "outputs = llm.chat(prompts, sampling_params)\n",
        "\n",
        "for output in outputs:\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(generated_text)"
      ],
      "metadata": {
        "id": "Mbsdue6Sis7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With 2 nodes of 8 GPUs"
      ],
      "metadata": {
        "id": "Na0drkpPjTKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "prompts = [{\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"},\n",
        "          {\"role\":\"user\", \"content\":\"Please add a pair of parentheses to the incorrect equation: 1 + 2 * 3 + 4 * 5 + 6 * 7 + 8 * 9 = 479, to make the equation true.\"}]\n",
        "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=5000)\n",
        "\n",
        "llm = LLM(model=\"deepseek-ai/DeepSeek-V3\", max_model_len=8192, tensor_parallel_size=8, pipeline_parallel_size=2)\n",
        "\n",
        "outputs = llm.chat(prompts, sampling_params)\n",
        "\n",
        "for output in outputs:\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(generated_text)"
      ],
      "metadata": {
        "id": "e_DYMKYojV5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Quantized Version"
      ],
      "metadata": {
        "id": "gCCJ37QSjbnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "prompts = [{\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"},\n",
        "          {\"role\":\"user\", \"content\":\"Please add a pair of parentheses to the incorrect equation: 1 + 2 * 3 + 4 * 5 + 6 * 7 + 8 * 9 = 479, to make the equation true.\"}]\n",
        "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=5000)\n",
        "\n",
        "llm = LLM(model=\"OPEA/DeepSeek-V3-int4-sym-gptq-inc\", max_model_len=8192, tensor_parallel_size=8)\n",
        "\n",
        "outputs = llm.chat(prompts, sampling_params)\n",
        "\n",
        "for output in outputs:\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(generated_text)"
      ],
      "metadata": {
        "id": "r4-vzV5IjbJG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}