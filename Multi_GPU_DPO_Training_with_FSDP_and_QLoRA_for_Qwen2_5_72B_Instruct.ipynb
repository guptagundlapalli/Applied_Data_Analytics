{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptagundlapalli/Applied_Data_Analytics/blob/master/Multi_GPU_DPO_Training_with_FSDP_and_QLoRA_for_Qwen2_5_72B_Instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*More details in this article: [Multi-GPU DPO Training with FSDP: Full Training, LoRA and QLoRA](https://kaitchup.substack.com/p/multi-gpu-dpo-training-with-fsdp)*\n",
        "\n",
        "\n",
        "This notebook shows how to train a 70B LLM, e.g., Qwen2.5 72B, using multiple GPUs. It exploits FSDP for multi-gpus training and QLoRA for parameter-efficient fine-tuning.\n",
        "\n",
        "This code runs on four 24 GB GPUs and requires at least 170 GB of CPU RAM.\n",
        "\n",
        "For supervised fine-tuning, the step before DPO training, check this article: [Multi-GPU Fine-tuning for Llama 3.1 70B with FSDP and QLoRA](https://kaitchup.substack.com/p/multi-gpus-fine-tuning-for-llama)\n",
        "\n",
        "\n",
        "*Note: This code was not tested with a Jupyter notebook. You may copy the training code into Python file and run this Python file with Accelerate.*\n",
        "\n",
        "\n",
        "First, we need to install:\n",
        "\n",
        "*Note: You need Transformers 4.46.3 (or more recent)*"
      ],
      "metadata": {
        "id": "A3L5I088PRr1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiarBXAHT-S1"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade bitsandbytes transformers peft accelerate datasets trl flash_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, configure Accelerate:"
      ],
      "metadata": {
        "id": "Bti-uyQpRO0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accelerate config"
      ],
      "metadata": {
        "id": "VW_4FMVNRTzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or use the following configuration file that you may copy into a file named \"config_fsdp.yaml\""
      ],
      "metadata": {
        "id": "pGOKHP4ZRVlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_environment: LOCAL_MACHINE\n",
        "debug: false\n",
        "distributed_type: FSDP\n",
        "downcast_bf16: 'no'\n",
        "fsdp_config:\n",
        "  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n",
        "  fsdp_backward_prefetch: BACKWARD_PRE\n",
        "  fsdp_cpu_ram_efficient_loading: true\n",
        "  fsdp_forward_prefetch: false\n",
        "  fsdp_offload_params: true\n",
        "  fsdp_sharding_strategy: FULL_SHARD\n",
        "  fsdp_state_dict_type: SHARDED_STATE_DICT\n",
        "  fsdp_sync_module_states: true\n",
        "  fsdp_use_orig_params: false\n",
        "machine_rank: 0\n",
        "main_training_function: main\n",
        "mixed_precision: 'no'\n",
        "num_machines: 1\n",
        "num_processes: 4\n",
        "rdzv_backend: static\n",
        "same_network: true\n",
        "tpu_env: []\n",
        "tpu_use_cluster: false\n",
        "tpu_use_sudo: false\n",
        "use_cpu: false"
      ],
      "metadata": {
        "id": "OI8pGYCSRjZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training code below must be run with accelerate. Copy it into a file, e.g., \"fsdp+QLoRA.py\" and then run\n",
        "\n",
        "\n",
        "```\n",
        "accelerate launch --config_file config_fsdp.yaml fsdp+QLoRA.py\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Exb6Q9dYQ0e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch, os, multiprocessing\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    set_seed\n",
        ")\n",
        "from peft.utils.other import fsdp_auto_wrap_policy\n",
        "from accelerate import Accelerator\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "accelerator = Accelerator()\n",
        "set_seed(1234)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-72B-Instruct\"\n",
        "sft_adapter = \"./SFT_LoRA/\" #a LoRA adapter fine-tuned with SFT\n",
        "\n",
        "compute_dtype = torch.bfloat16\n",
        "\n",
        "#If you have troubles with FlashAttention, use 'sdpa' instead\n",
        "attn_implementation = 'flash_attention_2'\n",
        "\n",
        "#Modify the following 3 training arguments if you run out of memory\n",
        "bs = 1 #Batch size per device (training and validation)\n",
        "gas = 16 #Gradient accumulation steps\n",
        "mseqlen = 512 #Maximum sequence length\n",
        "\n",
        "\n",
        "lr = 1e-5 #Learning rate\n",
        "QLoRA = True #Quantize the base model. I don't recommend it if you have enough memory to run LoRA\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.0\n",
        "lora_r = 16\n",
        "\n",
        "output_dir = \"/workspace/DPO_LoRA\"\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = \"<|image_pad|>\"\n",
        "tokenizer.pad_token_id = 151655\n",
        "tokenizer.padding_side = 'right' #right or left doesn't seem to matter for Qwen2.5 (which is not the case for Llama 3.1 which is better with right-padding for some reasons)\n",
        "\n",
        "#A dataset to test DPO training\n",
        "ds = load_dataset(\"mlabonne/orpo-dpo-mix-40k\", split=\"train\").train_test_split(test_size=0.01)\n",
        "ds_train = ds['train']\n",
        "ds_test = ds['test']\n",
        "\n",
        "#Add the EOS token\n",
        "def process(row):\n",
        "    #The first message is the prompt\n",
        "    prompt_messages = tokenizer.apply_chat_template([row[\"chosen\"][0]], tokenize=False)\n",
        "    chosen_messages = tokenizer.apply_chat_template(row[\"chosen\"][1:], tokenize=False)+tokenizer.eos_token\n",
        "    rejected_messages = tokenizer.apply_chat_template(row[\"rejected\"][1:], tokenize=False)+tokenizer.eos_token\n",
        "    row[\"prompt\"] = prompt_messages\n",
        "    row[\"chosen\"] = chosen_messages\n",
        "    row[\"rejected\"] = rejected_messages\n",
        "    return row\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    process,\n",
        "    num_proc= multiprocessing.cpu_count(),\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "\n",
        "ds_test = ds_test.map(\n",
        "    process,\n",
        "    num_proc= multiprocessing.cpu_count(),\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "\n",
        "\n",
        "if QLoRA:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_storage=compute_dtype,\n",
        "    )\n",
        "\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "              model_name, quantization_config=bnb_config, torch_dtype=torch.bfloat16, attn_implementation=attn_implementation\n",
        "    )\n",
        "    for name, param in model.named_parameters():\n",
        "    # freeze base model's layers\n",
        "        param.requires_grad = False\n",
        "    def make_inputs_require_grad(module, input, output):\n",
        "        output.requires_grad_(True)\n",
        "\n",
        "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "              model_name, torch_dtype=torch.bfloat16, attn_implementation=attn_implementation\n",
        "    )\n",
        "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':True})\n",
        "\n",
        "model = PeftModel.from_pretrained(model, sft_adapter, is_trainable=True, adapter_name=\"DPO\")\n",
        "model.load_adapter(sft_adapter, adapter_name=\"reference\")\n",
        "\n",
        "training_arguments = DPOConfig(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"steps\",\n",
        "        do_eval=True,\n",
        "        optim=\"adamw_torch\",\n",
        "        per_device_train_batch_size=bs,\n",
        "        gradient_accumulation_steps=gas,\n",
        "        per_device_eval_batch_size=bs,\n",
        "        log_level=\"debug\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=5,\n",
        "        logging_steps=2,\n",
        "        learning_rate=lr,\n",
        "        bf16 = True,\n",
        "        beta = 0.1,\n",
        "        eval_steps=2,\n",
        "        max_steps=10,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        max_length=mseqlen,\n",
        "        max_prompt_length=512,\n",
        "        dataset_num_proc=multiprocessing.cpu_count(),\n",
        "        model_adapter_name=\"DPO\",\n",
        "        ref_adapter_name=\"reference\",\n",
        ")\n",
        "\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_test,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "# LoRA's parameters are float32, we must downcast them to bfloat16\n",
        "# Necessary to flatten the tensors during model preparation by FSDP\n",
        "for param in model.parameters():\n",
        "     if (param.dtype == torch.float32):\n",
        "         param.data = param.data.to(torch.bfloat16)\n",
        "\n",
        "if trainer.ref_model is not None:\n",
        "    fsdp_plugin = trainer.accelerator.state.fsdp_plugin\n",
        "    fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.ref_model)\n",
        "    trainer.ref_model = trainer.accelerator.prepare_model(trainer.ref_model)\n",
        "\n",
        "fsdp_plugin = trainer.accelerator.state.fsdp_plugin\n",
        "fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)\n",
        "\n",
        "prepared_model = trainer._wrap_model(\n",
        "    trainer.model, training=True, dataloader=None\n",
        ")\n",
        "\n",
        "(\n",
        "    prepared_model,\n",
        "    trainer.optimizer,\n",
        "    trainer.lr_scheduler,\n",
        ") = trainer.accelerator.prepare(\n",
        "    prepared_model, trainer.optimizer, trainer.lr_scheduler\n",
        ")\n",
        "trainer.model_wrapped = prepared_model\n",
        "if trainer.is_fsdp_enabled:\n",
        "    trainer.model = prepared_model\n",
        "\n",
        "\n",
        "trainer.accelerator.prepare_model = lambda model, *args, **kwargs: model\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "if trainer.is_fsdp_enabled:\n",
        "    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
        "\n",
        "trainer.save_model(output_dir)"
      ],
      "metadata": {
        "id": "PfunHDnXXbiS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}